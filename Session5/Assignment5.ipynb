{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqEtbC7xNbB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyq8CE4ug5BK",
        "colab_type": "code",
        "outputId": "ffe3f9a2-c288-4104-b7b2-c680edd5fe8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "# mount gdrive and unzip data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip -q \"/content/gdrive/My Drive/EIP4/hvc_data.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir\n",
        "%ls "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  hvc_annotations.csv  \u001b[01;34mresized\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbNQzK6kj94",
        "colab_type": "code",
        "outputId": "2302c93b-d5ba-49cf-ebeb-b3d7330c2abe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "\n",
        "from keras.applications import VGG16\n",
        "from keras.applications.resnet50 import preprocess_input, ResNet50\n",
        "from keras.applications.resnet import ResNet152\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add, BatchNormalization\n",
        "from keras.layers import Convolution2D\n",
        "from keras.applications.vgg19 import VGG19\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9gP2BBWVrc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the minimum learning rate, maximum learning rate, batch size,\n",
        "# step size, CLR method, and number of epochs\n",
        "MIN_LR = 1e-7\n",
        "MAX_LR = 1e-2\n",
        "BATCH_SIZE = 32\n",
        "STEP_SIZE = 8\n",
        "CLR_METHOD = \"triangular\"\n",
        "NUM_EPOCHS = 96"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQkbSpLK4sTP",
        "colab_type": "code",
        "outputId": "f6185339-fd7e-41dd-f566-22dad42dab03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "\n",
        "# load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>imagequality</th>\n",
              "      <th>age</th>\n",
              "      <th>weight</th>\n",
              "      <th>carryingbag</th>\n",
              "      <th>footwear</th>\n",
              "      <th>emotion</th>\n",
              "      <th>bodypose</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>male</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>female</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>over-weight</td>\n",
              "      <td>None</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Angry/Serious</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>female</td>\n",
              "      <td>Good</td>\n",
              "      <td>35-45</td>\n",
              "      <td>slightly-overweight</td>\n",
              "      <td>None</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   gender imagequality    age  ...        emotion        bodypose     image_path\n",
              "0    male      Average  35-45  ...        Neutral  Front-Frontish  resized/1.jpg\n",
              "1  female      Average  35-45  ...  Angry/Serious  Front-Frontish  resized/2.jpg\n",
              "2    male         Good  45-55  ...        Neutral  Front-Frontish  resized/3.jpg\n",
              "3    male         Good  45-55  ...        Neutral  Front-Frontish  resized/4.jpg\n",
              "4  female         Good  35-45  ...        Neutral  Front-Frontish  resized/5.jpg\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "202OJva345WA",
        "colab_type": "code",
        "outputId": "b7b746aa-1a51-4f6b-815c-cc601b8d5dce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 888
        }
      },
      "source": [
        "\n",
        "\n",
        "# one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "one_hot_df.head().T"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>image_path</th>\n",
              "      <td>resized/1.jpg</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_female</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_male</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Average</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Good</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_15-25</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_25-35</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_35-45</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_45-55</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_55+</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_over-weight</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_underweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_None</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Normal</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Happy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Sad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Back</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Side</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  0  ...              4\n",
              "image_path                            resized/1.jpg  ...  resized/5.jpg\n",
              "gender_female                                     0  ...              1\n",
              "gender_male                                       1  ...              0\n",
              "imagequality_Average                              1  ...              0\n",
              "imagequality_Bad                                  0  ...              0\n",
              "imagequality_Good                                 0  ...              1\n",
              "age_15-25                                         0  ...              0\n",
              "age_25-35                                         0  ...              0\n",
              "age_35-45                                         1  ...              1\n",
              "age_45-55                                         0  ...              0\n",
              "age_55+                                           0  ...              0\n",
              "weight_normal-healthy                             1  ...              0\n",
              "weight_over-weight                                0  ...              0\n",
              "weight_slightly-overweight                        0  ...              1\n",
              "weight_underweight                                0  ...              0\n",
              "carryingbag_Daily/Office/Work Bag                 0  ...              0\n",
              "carryingbag_Grocery/Home/Plastic Bag              1  ...              0\n",
              "carryingbag_None                                  0  ...              1\n",
              "footwear_CantSee                                  0  ...              1\n",
              "footwear_Fancy                                    0  ...              0\n",
              "footwear_Normal                                   1  ...              0\n",
              "emotion_Angry/Serious                             0  ...              0\n",
              "emotion_Happy                                     0  ...              0\n",
              "emotion_Neutral                                   1  ...              1\n",
              "emotion_Sad                                       0  ...              0\n",
              "bodypose_Back                                     0  ...              0\n",
              "bodypose_Front-Frontish                           1  ...              1\n",
              "bodypose_Side                                     0  ...              0\n",
              "\n",
              "[28 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ll94zTv6w5i",
        "colab_type": "code",
        "outputId": "cea10cfa-f186-4a64-d15f-b95f101ad5c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
        "print(_gender_cols_)\n",
        "\n",
        "class PersonDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    \n",
        "    def __init__(self, df, batch_size=64, augmentation=False, shuffle=False):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        self.augmentation = augmentation\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        image = np.stack([cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()])\n",
        "        if self.augmentation is not None:\n",
        "          image = self.augmentation.flow(image, shuffle=False).next()\n",
        "          target = {\n",
        "                \"gender_output\": items[_gender_cols_].values,\n",
        "                \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "                \"age_output\": items[_age_cols_].values,\n",
        "                \"weight_output\": items[_weight_cols_].values,\n",
        "                \"bag_output\": items[_carryingbag_cols_].values,\n",
        "                \"pose_output\": items[_bodypose_cols_].values,\n",
        "                \"footwear_output\": items[_footwear_cols_].values,\n",
        "                \"emotion_output\": items[_emotion_cols_].values,\n",
        "            }\n",
        "        return image, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['gender_female', 'gender_male']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVE8-OaZ8J5q",
        "colab_type": "code",
        "outputId": "a470d0a9-ec74-4436-8096-797ad120621b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.15)\n",
        "train_df.shape, val_df.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11537, 28), (2036, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5m15DLyF2ot",
        "colab_type": "code",
        "outputId": "d3bb0ed7-7f08-400f-ff45-6d4deca8a3b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "train_df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>gender_female</th>\n",
              "      <th>gender_male</th>\n",
              "      <th>imagequality_Average</th>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <th>imagequality_Good</th>\n",
              "      <th>age_15-25</th>\n",
              "      <th>age_25-35</th>\n",
              "      <th>age_35-45</th>\n",
              "      <th>age_45-55</th>\n",
              "      <th>age_55+</th>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <th>weight_over-weight</th>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <th>weight_underweight</th>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <th>carryingbag_None</th>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <th>footwear_Normal</th>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <th>emotion_Happy</th>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <th>emotion_Sad</th>\n",
              "      <th>bodypose_Back</th>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <th>bodypose_Side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9756</th>\n",
              "      <td>resized/9757.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6487</th>\n",
              "      <td>resized/6488.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12013</th>\n",
              "      <td>resized/12015.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7825</th>\n",
              "      <td>resized/7826.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12758</th>\n",
              "      <td>resized/12760.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
              "9756    resized/9757.jpg              0  ...                        1              0\n",
              "6487    resized/6488.jpg              1  ...                        1              0\n",
              "12013  resized/12015.jpg              0  ...                        0              1\n",
              "7825    resized/7826.jpg              0  ...                        1              0\n",
              "12758  resized/12760.jpg              0  ...                        1              0\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTiOi5tVBnhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# create train and validation data generators\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=32, augmentation=ImageDataGenerator(\n",
        "horizontal_flip=True,\n",
        "vertical_flip=True))\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=32, augmentation=ImageDataGenerator(\n",
        "horizontal_flip=True,\n",
        "vertical_flip=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHp16Kv57hsI",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pMDGat-Ghow",
        "colab_type": "code",
        "outputId": "ded4bdb1-a038-425e-c81d-4adc2b683890",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "\n",
        "\n",
        "# get number of output units from data\n",
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "num_units"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age': 5,\n",
              " 'bag': 3,\n",
              " 'emotion': 4,\n",
              " 'footwear': 3,\n",
              " 'gender': 2,\n",
              " 'image_quality': 3,\n",
              " 'pose': 3,\n",
              " 'weight': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03W8Pagg_Ppp",
        "colab_type": "code",
        "outputId": "97227d3d-1f63-4e6c-f60b-5d6c4473f266",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "source": [
        "\n",
        "\n",
        "# backbone = VGG16(\n",
        "#     weights=\"imagenet\", \n",
        "#     include_top=False, \n",
        "#     input_tensor=Input(shape=(224, 224, 3))\n",
        "# )\n",
        "# backbone = ResNet50( \n",
        "#     include_top=False,\n",
        "#     weights = None, \n",
        "#     input_tensor=Input(shape=(224, 224, 3))\n",
        "# )\n",
        "backbone = VGG19( \n",
        "    include_top=False,\n",
        "    weights = None, \n",
        "    input_tensor=Input(shape=(224, 224, 3))\n",
        ")\n",
        "\n",
        "\n",
        "neck = backbone.output\n",
        "neck = Flatten(name=\"flatten\")(neck)\n",
        "#neck = Dense(512, activation=\"relu\")(neck)\n",
        "neck  = Dense(512, activation=\"relu\")(neck)\n",
        "\n",
        "\n",
        "def build_tower(in_layer):\n",
        "    neck = Dropout(0.2)(in_layer)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    #neck = Dropout(0.3)(in_layer)\n",
        "    #neck = Dense(128, activation=\"relu\")(neck)\n",
        "    return neck\n",
        "\n",
        "\n",
        "def build_head(name, in_layer):\n",
        "    return Dense(\n",
        "        num_units[name],  activation=\"softmax\", name=f\"{name}_output\"\n",
        "    )(in_layer)\n",
        "\n",
        "# heads\n",
        "gender = build_head(\"gender\", build_tower(neck))\n",
        "image_quality = build_head(\"image_quality\", build_tower(neck))\n",
        "age = build_head(\"age\", build_tower(neck))\n",
        "weight = build_head(\"weight\", build_tower(neck))\n",
        "bag = build_head(\"bag\", build_tower(neck))\n",
        "footwear = build_head(\"footwear\", build_tower(neck))\n",
        "emotion = build_head(\"emotion\", build_tower(neck))\n",
        "pose = build_head(\"pose\", build_tower(neck))\n",
        "\n",
        "\n",
        "model = Model(\n",
        "    inputs=backbone.input, \n",
        "    outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion]\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxWVxcbi_y6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# freeze backbone\n",
        "for layer in backbone.layers:\n",
        "\tlayer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opXI-9ZqA87M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# epoch = 200\n",
        "# def lr_schedule(epoch):\n",
        "#     \"\"\"Learning Rate Schedule\n",
        "\n",
        "#     Learning rate is scheduled to be reduced after 20, 35, 70, 90 epochs.\n",
        "#     Called automatically every epoch as part of callbacks during training.\n",
        "\n",
        "#     # Arguments\n",
        "#         epoch (int): The number of epochs\n",
        "\n",
        "#     # Returns\n",
        "#         lr (float32): learning rate\n",
        "#     \"\"\"\n",
        "#     lr = 0.01\n",
        "#     if epoch > 20:\n",
        "#         lr *= 1e-1\n",
        "#     elif epoch > 35:\n",
        "#         lr *= 1e-3\n",
        "#     elif epoch > 70:\n",
        "#         lr *= 1e-2\n",
        "#     elif epoch > 90:\n",
        "#         lr *= 1e-1\n",
        "#     elif epoch > 150:\n",
        "#       lr *= 1e-1\n",
        "    \n",
        "#     print('Learning rate: ', lr)\n",
        "#     return lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrFh_RXMBshr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfPG9C2eA1zn",
        "colab_type": "code",
        "outputId": "9c141245-3b6d-4602-92d6-99f5864c0857",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"[INFO] compiling model...\")\n",
        "opt = SGD(lr=MIN_LR, momentum=0.9)\n",
        "#model = MiniGoogLeNet.build(width=32, height=32, depth=3, classes=10)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
        "\tmetrics=[\"accuracy\"])\n",
        " \n",
        "\n",
        "# }\n",
        "# # loss_weights = {\"gender_output\": 1.0, \"image_quality_output\": 1.0, \"age_output\": 1.0}\n",
        "# import random\n",
        "#opt = SGD(lr=lr_schedule(epoch),  momentum=0.9)\n",
        "#opt = SGD(lr=0.01, momentum=0.9, decay=0.1)\n",
        "# model.compile(\n",
        "#     optimizer=opt,\n",
        "#     loss=\"categorical_crossentropy\", \n",
        "#     # loss_weights=loss_weights, \n",
        "#     metrics=[\"accuracy\"]\n",
        "# )\n",
        "\n",
        "# model.compile(optimizer=opt,\n",
        "#               loss='categorical_crossentropy',\n",
        "#               #optimizer='adam',\n",
        "#               metrics=['accuracy'])\n",
        "#model.summary()\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] compiling model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qD9GHNJCcwdr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import Callback\n",
        "from keras import backend as K\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class CyclicLR(Callback):\n",
        "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
        "    The method cycles the learning rate between two boundaries with\n",
        "    some constant frequency.\n",
        "    # Arguments\n",
        "        base_lr: initial learning rate which is the\n",
        "            lower boundary in the cycle.\n",
        "        max_lr: upper boundary in the cycle. Functionally,\n",
        "            it defines the cycle amplitude (max_lr - base_lr).\n",
        "            The lr at any cycle is the sum of base_lr\n",
        "            and some scaling of the amplitude; therefore\n",
        "            max_lr may not actually be reached depending on\n",
        "            scaling function.\n",
        "        step_size: number of training iterations per\n",
        "            half cycle. Authors suggest setting step_size\n",
        "            2-8 x training iterations in epoch.\n",
        "        mode: one of {triangular, triangular2, exp_range}.\n",
        "            Default 'triangular'.\n",
        "            Values correspond to policies detailed above.\n",
        "            If scale_fn is not None, this argument is ignored.\n",
        "        gamma: constant in 'exp_range' scaling function:\n",
        "            gamma**(cycle iterations)\n",
        "        scale_fn: Custom scaling policy defined by a single\n",
        "            argument lambda function, where\n",
        "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
        "            mode paramater is ignored\n",
        "        scale_mode: {'cycle', 'iterations'}.\n",
        "            Defines whether scale_fn is evaluated on\n",
        "            cycle number or cycle iterations (training\n",
        "            iterations since start of cycle). Default is 'cycle'.\n",
        "\n",
        "    The amplitude of the cycle can be scaled on a per-iteration or\n",
        "    per-cycle basis.\n",
        "    This class has three built-in policies, as put forth in the paper.\n",
        "    \"triangular\":\n",
        "        A basic triangular cycle w/ no amplitude scaling.\n",
        "    \"triangular2\":\n",
        "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
        "    \"exp_range\":\n",
        "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each\n",
        "        cycle iteration.\n",
        "    For more detail, please see paper.\n",
        "\n",
        "    # Example for CIFAR-10 w/ batch size 100:\n",
        "        ```python\n",
        "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
        "                                step_size=2000., mode='triangular')\n",
        "            model.fit(X_train, Y_train, callbacks=[clr])\n",
        "        ```\n",
        "\n",
        "    Class also supports custom scaling functions:\n",
        "        ```python\n",
        "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
        "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
        "                                step_size=2000., scale_fn=clr_fn,\n",
        "                                scale_mode='cycle')\n",
        "            model.fit(X_train, Y_train, callbacks=[clr])\n",
        "        ```\n",
        "\n",
        "    # References\n",
        "\n",
        "      - [Cyclical Learning Rates for Training Neural Networks](\n",
        "      https://arxiv.org/abs/1506.01186)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            base_lr=0.001,\n",
        "            max_lr=0.006,\n",
        "            step_size=2000.,\n",
        "            mode='triangular',\n",
        "            gamma=1.,\n",
        "            scale_fn=None,\n",
        "            scale_mode='cycle'):\n",
        "        super(CyclicLR, self).__init__()\n",
        "\n",
        "        if mode not in ['triangular', 'triangular2',\n",
        "                        'exp_range']:\n",
        "            raise KeyError(\"mode must be one of 'triangular', \"\n",
        "                           \"'triangular2', or 'exp_range'\")\n",
        "        self.base_lr = base_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.step_size = step_size\n",
        "        self.mode = mode\n",
        "        self.gamma = gamma\n",
        "        if scale_fn is None:\n",
        "            if self.mode == 'triangular':\n",
        "                self.scale_fn = lambda x: 1.\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'triangular2':\n",
        "                self.scale_fn = lambda x: 1 / (2.**(x - 1))\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'exp_range':\n",
        "                self.scale_fn = lambda x: gamma ** x\n",
        "                self.scale_mode = 'iterations'\n",
        "        else:\n",
        "            self.scale_fn = scale_fn\n",
        "            self.scale_mode = scale_mode\n",
        "        self.clr_iterations = 0.\n",
        "        self.trn_iterations = 0.\n",
        "        self.history = {}\n",
        "\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
        "               new_step_size=None):\n",
        "        \"\"\"Resets cycle iterations.\n",
        "        Optional boundary/step size adjustment.\n",
        "        \"\"\"\n",
        "        if new_base_lr is not None:\n",
        "            self.base_lr = new_base_lr\n",
        "        if new_max_lr is not None:\n",
        "            self.max_lr = new_max_lr\n",
        "        if new_step_size is not None:\n",
        "            self.step_size = new_step_size\n",
        "        self.clr_iterations = 0.\n",
        "\n",
        "    def clr(self):\n",
        "        cycle = np.floor(1 + self.clr_iterations / (2 * self.step_size))\n",
        "        x = np.abs(self.clr_iterations / self.step_size - 2 * cycle + 1)\n",
        "        if self.scale_mode == 'cycle':\n",
        "            return self.base_lr + (self.max_lr - self.base_lr) * \\\n",
        "                np.maximum(0, (1 - x)) * self.scale_fn(cycle)\n",
        "        else:\n",
        "            return self.base_lr + (self.max_lr - self.base_lr) * \\\n",
        "                np.maximum(0, (1 - x)) * self.scale_fn(self.clr_iterations)\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        logs = logs or {}\n",
        "\n",
        "        if self.clr_iterations == 0:\n",
        "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.clr())\n",
        "\n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "\n",
        "        logs = logs or {}\n",
        "        self.trn_iterations += 1\n",
        "        self.clr_iterations += 1\n",
        "        K.set_value(self.model.optimizer.lr, self.clr())\n",
        "\n",
        "        self.history.setdefault(\n",
        "            'lr', []).append(\n",
        "            K.get_value(\n",
        "                self.model.optimizer.lr))\n",
        "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
        "\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        logs['lr'] = K.get_value(self.model.optimizer.lr)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5O9GfeNZkVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "import os\n",
        "# Prepare model model saving directory.\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'model.{epoch:03d}.h5' \n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_age_output_acc', \n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "# checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "#                              monitor='val_gender_output_acc', 'val_age_output_acc'\n",
        "#                              verbose=1,\n",
        "#                              save_best_only=True)\n",
        "\n",
        "# lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "# lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "#                                cooldown=0,\n",
        "#                                patience=5,\n",
        "#                                min_lr=0.5e-6)\n",
        "\n",
        "# initialize the cyclical learning rate callback\n",
        "clr = CyclicLR(\n",
        "\tmode=CLR_METHOD,\n",
        "\tbase_lr=MIN_LR,\n",
        "\tmax_lr=MAX_LR,\n",
        "\tstep_size= STEP_SIZE * (train_df.shape[0] // BATCH_SIZE))\n",
        "\n",
        "callbacks = [checkpoint, clr]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpxv41EyNmN4",
        "colab_type": "code",
        "outputId": "960cfe9c-5c6f-4707-d876-7d3e9e1e10e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True, \n",
        "    epochs=NUM_EPOCHS,\n",
        "    verbose=1,\n",
        "    callbacks=[clr]\n",
        ")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/96\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.6884 - gender_output_loss: 0.6809 - image_quality_output_loss: 0.9732 - age_output_loss: 1.4192 - weight_output_loss: 0.9790 - bag_output_loss: 0.9111 - footwear_output_loss: 0.9016 - pose_output_loss: 0.9164 - emotion_output_loss: 0.9070 - gender_output_acc: 0.5664 - image_quality_output_acc: 0.5538 - age_output_acc: 0.4012 - weight_output_acc: 0.6355 - bag_output_acc: 0.5649 - footwear_output_acc: 0.5939 - pose_output_acc: 0.6191 - emotion_output_acc: 0.7098Epoch 1/96\n",
            "360/360 [==============================] - 55s 152ms/step - loss: 7.6900 - gender_output_loss: 0.6809 - image_quality_output_loss: 0.9733 - age_output_loss: 1.4196 - weight_output_loss: 0.9790 - bag_output_loss: 0.9114 - footwear_output_loss: 0.9022 - pose_output_loss: 0.9163 - emotion_output_loss: 0.9073 - gender_output_acc: 0.5664 - image_quality_output_acc: 0.5539 - age_output_acc: 0.4009 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5938 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7192 - val_gender_output_loss: 0.6852 - val_image_quality_output_loss: 0.9818 - val_age_output_loss: 1.4387 - val_weight_output_loss: 0.9767 - val_bag_output_loss: 0.9259 - val_footwear_output_loss: 0.8973 - val_pose_output_loss: 0.9330 - val_emotion_output_loss: 0.8805 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5441 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5977 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 2/96\n",
            "360/360 [==============================] - 55s 153ms/step - loss: 7.6934 - gender_output_loss: 0.6817 - image_quality_output_loss: 0.9739 - age_output_loss: 1.4202 - weight_output_loss: 0.9794 - bag_output_loss: 0.9121 - footwear_output_loss: 0.9030 - pose_output_loss: 0.9157 - emotion_output_loss: 0.9074 - gender_output_acc: 0.5656 - image_quality_output_acc: 0.5555 - age_output_acc: 0.4004 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5950 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7293 - val_gender_output_loss: 0.6856 - val_image_quality_output_loss: 0.9808 - val_age_output_loss: 1.4386 - val_weight_output_loss: 0.9762 - val_bag_output_loss: 0.9262 - val_footwear_output_loss: 0.9095 - val_pose_output_loss: 0.9322 - val_emotion_output_loss: 0.8804 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5446 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5987 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 3/96\n",
            "360/360 [==============================] - 54s 151ms/step - loss: 7.7104 - gender_output_loss: 0.6826 - image_quality_output_loss: 0.9739 - age_output_loss: 1.4208 - weight_output_loss: 0.9791 - bag_output_loss: 0.9120 - footwear_output_loss: 0.9167 - pose_output_loss: 0.9176 - emotion_output_loss: 0.9076 - gender_output_acc: 0.5664 - image_quality_output_acc: 0.5534 - age_output_acc: 0.4001 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5811 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7810 - val_gender_output_loss: 0.6892 - val_image_quality_output_loss: 0.9832 - val_age_output_loss: 1.4415 - val_weight_output_loss: 0.9771 - val_bag_output_loss: 0.9277 - val_footwear_output_loss: 0.9464 - val_pose_output_loss: 0.9354 - val_emotion_output_loss: 0.8804 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5441 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5689 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 4/96\n",
            "360/360 [==============================] - 54s 150ms/step - loss: 7.7201 - gender_output_loss: 0.6824 - image_quality_output_loss: 0.9750 - age_output_loss: 1.4219 - weight_output_loss: 0.9798 - bag_output_loss: 0.9127 - footwear_output_loss: 0.9206 - pose_output_loss: 0.9193 - emotion_output_loss: 0.9082 - gender_output_acc: 0.5648 - image_quality_output_acc: 0.5554 - age_output_acc: 0.4007 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5774 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7375 - val_gender_output_loss: 0.6859 - val_image_quality_output_loss: 0.9818 - val_age_output_loss: 1.4390 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9270 - val_footwear_output_loss: 0.9091 - val_pose_output_loss: 0.9343 - val_emotion_output_loss: 0.8819 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5441 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5942 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 5/96\n",
            "360/360 [==============================] - 54s 151ms/step - loss: 7.7168 - gender_output_loss: 0.6825 - image_quality_output_loss: 0.9746 - age_output_loss: 1.4220 - weight_output_loss: 0.9806 - bag_output_loss: 0.9124 - footwear_output_loss: 0.9187 - pose_output_loss: 0.9186 - emotion_output_loss: 0.9075 - gender_output_acc: 0.5649 - image_quality_output_acc: 0.5556 - age_output_acc: 0.4003 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5847 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.8021 - val_gender_output_loss: 0.6883 - val_image_quality_output_loss: 0.9878 - val_age_output_loss: 1.4409 - val_weight_output_loss: 0.9787 - val_bag_output_loss: 0.9279 - val_footwear_output_loss: 0.9582 - val_pose_output_loss: 0.9390 - val_emotion_output_loss: 0.8814 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5491 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 6/96\n",
            "360/360 [==============================] - 54s 150ms/step - loss: 7.7198 - gender_output_loss: 0.6828 - image_quality_output_loss: 0.9748 - age_output_loss: 1.4218 - weight_output_loss: 0.9808 - bag_output_loss: 0.9134 - footwear_output_loss: 0.9179 - pose_output_loss: 0.9198 - emotion_output_loss: 0.9085 - gender_output_acc: 0.5655 - image_quality_output_acc: 0.5541 - age_output_acc: 0.4006 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5821 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7602 - val_gender_output_loss: 0.6883 - val_image_quality_output_loss: 0.9813 - val_age_output_loss: 1.4419 - val_weight_output_loss: 0.9787 - val_bag_output_loss: 0.9273 - val_footwear_output_loss: 0.9236 - val_pose_output_loss: 0.9367 - val_emotion_output_loss: 0.8823 - val_gender_output_acc: 0.5446 - val_image_quality_output_acc: 0.5441 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5650 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 7/96\n",
            "360/360 [==============================] - 54s 151ms/step - loss: 7.7267 - gender_output_loss: 0.6836 - image_quality_output_loss: 0.9760 - age_output_loss: 1.4226 - weight_output_loss: 0.9807 - bag_output_loss: 0.9129 - footwear_output_loss: 0.9203 - pose_output_loss: 0.9213 - emotion_output_loss: 0.9095 - gender_output_acc: 0.5648 - image_quality_output_acc: 0.5539 - age_output_acc: 0.4003 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5848 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7916 - val_gender_output_loss: 0.6867 - val_image_quality_output_loss: 0.9852 - val_age_output_loss: 1.4444 - val_weight_output_loss: 0.9801 - val_bag_output_loss: 0.9275 - val_footwear_output_loss: 0.9473 - val_pose_output_loss: 0.9387 - val_emotion_output_loss: 0.8818 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5441 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5610 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 8/96\n",
            "360/360 [==============================] - 54s 151ms/step - loss: 7.7219 - gender_output_loss: 0.6832 - image_quality_output_loss: 0.9759 - age_output_loss: 1.4222 - weight_output_loss: 0.9798 - bag_output_loss: 0.9127 - footwear_output_loss: 0.9178 - pose_output_loss: 0.9214 - emotion_output_loss: 0.9089 - gender_output_acc: 0.5650 - image_quality_output_acc: 0.5550 - age_output_acc: 0.3996 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5820 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7496 - val_gender_output_loss: 0.6869 - val_image_quality_output_loss: 0.9859 - val_age_output_loss: 1.4395 - val_weight_output_loss: 0.9782 - val_bag_output_loss: 0.9292 - val_footwear_output_loss: 0.9098 - val_pose_output_loss: 0.9392 - val_emotion_output_loss: 0.8808 - val_gender_output_acc: 0.5496 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5923 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 9/96\n",
            "360/360 [==============================] - 54s 151ms/step - loss: 7.7438 - gender_output_loss: 0.6835 - image_quality_output_loss: 0.9762 - age_output_loss: 1.4234 - weight_output_loss: 0.9810 - bag_output_loss: 0.9138 - footwear_output_loss: 0.9361 - pose_output_loss: 0.9213 - emotion_output_loss: 0.9086 - gender_output_acc: 0.5649 - image_quality_output_acc: 0.5553 - age_output_acc: 0.3999 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5726 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.8225 - val_gender_output_loss: 0.6907 - val_image_quality_output_loss: 0.9837 - val_age_output_loss: 1.4411 - val_weight_output_loss: 0.9811 - val_bag_output_loss: 0.9286 - val_footwear_output_loss: 0.9759 - val_pose_output_loss: 0.9390 - val_emotion_output_loss: 0.8824 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5441 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5337 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 10/96\n",
            "360/360 [==============================] - 54s 150ms/step - loss: 7.7396 - gender_output_loss: 0.6832 - image_quality_output_loss: 0.9762 - age_output_loss: 1.4238 - weight_output_loss: 0.9811 - bag_output_loss: 0.9135 - footwear_output_loss: 0.9314 - pose_output_loss: 0.9216 - emotion_output_loss: 0.9086 - gender_output_acc: 0.5655 - image_quality_output_acc: 0.5557 - age_output_acc: 0.4005 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5709 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7589 - val_gender_output_loss: 0.6865 - val_image_quality_output_loss: 0.9844 - val_age_output_loss: 1.4404 - val_weight_output_loss: 0.9774 - val_bag_output_loss: 0.9260 - val_footwear_output_loss: 0.9216 - val_pose_output_loss: 0.9401 - val_emotion_output_loss: 0.8826 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5863 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 11/96\n",
            "360/360 [==============================] - 54s 150ms/step - loss: 7.7156 - gender_output_loss: 0.6830 - image_quality_output_loss: 0.9747 - age_output_loss: 1.4214 - weight_output_loss: 0.9804 - bag_output_loss: 0.9125 - footwear_output_loss: 0.9133 - pose_output_loss: 0.9216 - emotion_output_loss: 0.9087 - gender_output_acc: 0.5640 - image_quality_output_acc: 0.5544 - age_output_acc: 0.4007 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5844 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7421 - val_gender_output_loss: 0.6850 - val_image_quality_output_loss: 0.9836 - val_age_output_loss: 1.4407 - val_weight_output_loss: 0.9785 - val_bag_output_loss: 0.9267 - val_footwear_output_loss: 0.9097 - val_pose_output_loss: 0.9362 - val_emotion_output_loss: 0.8817 - val_gender_output_acc: 0.5427 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5928 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 12/96\n",
            "360/360 [==============================] - 54s 150ms/step - loss: 7.7024 - gender_output_loss: 0.6819 - image_quality_output_loss: 0.9750 - age_output_loss: 1.4210 - weight_output_loss: 0.9795 - bag_output_loss: 0.9118 - footwear_output_loss: 0.9082 - pose_output_loss: 0.9175 - emotion_output_loss: 0.9076 - gender_output_acc: 0.5642 - image_quality_output_acc: 0.5556 - age_output_acc: 0.4005 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5895 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7456 - val_gender_output_loss: 0.6854 - val_image_quality_output_loss: 0.9832 - val_age_output_loss: 1.4413 - val_weight_output_loss: 0.9776 - val_bag_output_loss: 0.9259 - val_footwear_output_loss: 0.9162 - val_pose_output_loss: 0.9346 - val_emotion_output_loss: 0.8814 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5441 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5893 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 13/96\n",
            "360/360 [==============================] - 54s 150ms/step - loss: 7.6994 - gender_output_loss: 0.6823 - image_quality_output_loss: 0.9731 - age_output_loss: 1.4208 - weight_output_loss: 0.9797 - bag_output_loss: 0.9118 - footwear_output_loss: 0.9068 - pose_output_loss: 0.9173 - emotion_output_loss: 0.9077 - gender_output_acc: 0.5647 - image_quality_output_acc: 0.5549 - age_output_acc: 0.4008 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5864 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7397 - val_gender_output_loss: 0.6847 - val_image_quality_output_loss: 0.9804 - val_age_output_loss: 1.4396 - val_weight_output_loss: 0.9796 - val_bag_output_loss: 0.9261 - val_footwear_output_loss: 0.9149 - val_pose_output_loss: 0.9332 - val_emotion_output_loss: 0.8811 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5451 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5863 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 14/96\n",
            "360/360 [==============================] - 54s 150ms/step - loss: 7.6885 - gender_output_loss: 0.6812 - image_quality_output_loss: 0.9736 - age_output_loss: 1.4204 - weight_output_loss: 0.9796 - bag_output_loss: 0.9111 - footwear_output_loss: 0.8982 - pose_output_loss: 0.9172 - emotion_output_loss: 0.9073 - gender_output_acc: 0.5661 - image_quality_output_acc: 0.5541 - age_output_acc: 0.4007 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5941 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7197 - val_gender_output_loss: 0.6866 - val_image_quality_output_loss: 0.9814 - val_age_output_loss: 1.4383 - val_weight_output_loss: 0.9763 - val_bag_output_loss: 0.9248 - val_footwear_output_loss: 0.8991 - val_pose_output_loss: 0.9332 - val_emotion_output_loss: 0.8802 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5451 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5853 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "\n",
            "Epoch 15/96\n",
            "360/360 [==============================] - 54s 150ms/step - loss: 7.6805 - gender_output_loss: 0.6813 - image_quality_output_loss: 0.9727 - age_output_loss: 1.4207 - weight_output_loss: 0.9787 - bag_output_loss: 0.9117 - footwear_output_loss: 0.8927 - pose_output_loss: 0.9153 - emotion_output_loss: 0.9075 - gender_output_acc: 0.5661 - image_quality_output_acc: 0.5556 - age_output_acc: 0.4006 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.6017 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7194 - val_gender_output_loss: 0.6837 - val_image_quality_output_loss: 0.9818 - val_age_output_loss: 1.4395 - val_weight_output_loss: 0.9760 - val_bag_output_loss: 0.9254 - val_footwear_output_loss: 0.8990 - val_pose_output_loss: 0.9333 - val_emotion_output_loss: 0.8806 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5446 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.6012 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 16/96\n",
            "360/360 [==============================] - 54s 149ms/step - loss: 7.6734 - gender_output_loss: 0.6809 - image_quality_output_loss: 0.9726 - age_output_loss: 1.4207 - weight_output_loss: 0.9784 - bag_output_loss: 0.9112 - footwear_output_loss: 0.8879 - pose_output_loss: 0.9149 - emotion_output_loss: 0.9067 - gender_output_acc: 0.5661 - image_quality_output_acc: 0.5555 - age_output_acc: 0.4011 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.6035 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7117 - val_gender_output_loss: 0.6841 - val_image_quality_output_loss: 0.9803 - val_age_output_loss: 1.4384 - val_weight_output_loss: 0.9759 - val_bag_output_loss: 0.9249 - val_footwear_output_loss: 0.8949 - val_pose_output_loss: 0.9325 - val_emotion_output_loss: 0.8807 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5446 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5992 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 17/96\n",
            "360/360 [==============================] - 54s 151ms/step - loss: 7.6711 - gender_output_loss: 0.6807 - image_quality_output_loss: 0.9721 - age_output_loss: 1.4198 - weight_output_loss: 0.9782 - bag_output_loss: 0.9112 - footwear_output_loss: 0.8890 - pose_output_loss: 0.9138 - emotion_output_loss: 0.9064 - gender_output_acc: 0.5661 - image_quality_output_acc: 0.5552 - age_output_acc: 0.4006 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.6002 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7363 - val_gender_output_loss: 0.6845 - val_image_quality_output_loss: 0.9812 - val_age_output_loss: 1.4400 - val_weight_output_loss: 0.9767 - val_bag_output_loss: 0.9258 - val_footwear_output_loss: 0.9147 - val_pose_output_loss: 0.9325 - val_emotion_output_loss: 0.8810 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5441 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5938 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 18/96\n",
            "360/360 [==============================] - 55s 152ms/step - loss: 7.6750 - gender_output_loss: 0.6810 - image_quality_output_loss: 0.9727 - age_output_loss: 1.4202 - weight_output_loss: 0.9786 - bag_output_loss: 0.9111 - footwear_output_loss: 0.8894 - pose_output_loss: 0.9155 - emotion_output_loss: 0.9066 - gender_output_acc: 0.5663 - image_quality_output_acc: 0.5545 - age_output_acc: 0.4006 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.6015 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7089 - val_gender_output_loss: 0.6844 - val_image_quality_output_loss: 0.9819 - val_age_output_loss: 1.4379 - val_weight_output_loss: 0.9752 - val_bag_output_loss: 0.9246 - val_footwear_output_loss: 0.8927 - val_pose_output_loss: 0.9320 - val_emotion_output_loss: 0.8803 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5446 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.6022 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 19/96\n",
            "360/360 [==============================] - 54s 151ms/step - loss: 7.6861 - gender_output_loss: 0.6810 - image_quality_output_loss: 0.9731 - age_output_loss: 1.4209 - weight_output_loss: 0.9785 - bag_output_loss: 0.9113 - footwear_output_loss: 0.8985 - pose_output_loss: 0.9161 - emotion_output_loss: 0.9067 - gender_output_acc: 0.5654 - image_quality_output_acc: 0.5560 - age_output_acc: 0.4007 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5968 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7252 - val_gender_output_loss: 0.6826 - val_image_quality_output_loss: 0.9797 - val_age_output_loss: 1.4401 - val_weight_output_loss: 0.9744 - val_bag_output_loss: 0.9264 - val_footwear_output_loss: 0.9095 - val_pose_output_loss: 0.9319 - val_emotion_output_loss: 0.8805 - val_gender_output_acc: 0.5451 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5913 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 20/96\n",
            "360/360 [==============================] - 54s 150ms/step - loss: 7.7026 - gender_output_loss: 0.6822 - image_quality_output_loss: 0.9734 - age_output_loss: 1.4206 - weight_output_loss: 0.9787 - bag_output_loss: 0.9131 - footwear_output_loss: 0.9102 - pose_output_loss: 0.9170 - emotion_output_loss: 0.9073 - gender_output_acc: 0.5657 - image_quality_output_acc: 0.5554 - age_output_acc: 0.4003 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5902 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7439 - val_gender_output_loss: 0.6882 - val_image_quality_output_loss: 0.9868 - val_age_output_loss: 1.4383 - val_weight_output_loss: 0.9766 - val_bag_output_loss: 0.9269 - val_footwear_output_loss: 0.9094 - val_pose_output_loss: 0.9368 - val_emotion_output_loss: 0.8809 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5441 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5938 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 21/96\n",
            "360/360 [==============================] - 54s 150ms/step - loss: 7.7160 - gender_output_loss: 0.6827 - image_quality_output_loss: 0.9744 - age_output_loss: 1.4220 - weight_output_loss: 0.9800 - bag_output_loss: 0.9132 - footwear_output_loss: 0.9176 - pose_output_loss: 0.9181 - emotion_output_loss: 0.9080 - gender_output_acc: 0.5653 - image_quality_output_acc: 0.5532 - age_output_acc: 0.4001 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5830 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7972 - val_gender_output_loss: 0.6904 - val_image_quality_output_loss: 0.9856 - val_age_output_loss: 1.4413 - val_weight_output_loss: 0.9801 - val_bag_output_loss: 0.9277 - val_footwear_output_loss: 0.9521 - val_pose_output_loss: 0.9391 - val_emotion_output_loss: 0.8809 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5441 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5605 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 22/96\n",
            "360/360 [==============================] - 54s 150ms/step - loss: 7.7147 - gender_output_loss: 0.6823 - image_quality_output_loss: 0.9748 - age_output_loss: 1.4221 - weight_output_loss: 0.9796 - bag_output_loss: 0.9132 - footwear_output_loss: 0.9143 - pose_output_loss: 0.9200 - emotion_output_loss: 0.9085 - gender_output_acc: 0.5661 - image_quality_output_acc: 0.5540 - age_output_acc: 0.4003 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5830 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7549 - val_gender_output_loss: 0.6831 - val_image_quality_output_loss: 0.9800 - val_age_output_loss: 1.4416 - val_weight_output_loss: 0.9795 - val_bag_output_loss: 0.9280 - val_footwear_output_loss: 0.9272 - val_pose_output_loss: 0.9338 - val_emotion_output_loss: 0.8816 - val_gender_output_acc: 0.5451 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5779 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 23/96\n",
            "360/360 [==============================] - 54s 151ms/step - loss: 7.7155 - gender_output_loss: 0.6825 - image_quality_output_loss: 0.9747 - age_output_loss: 1.4216 - weight_output_loss: 0.9801 - bag_output_loss: 0.9124 - footwear_output_loss: 0.9183 - pose_output_loss: 0.9179 - emotion_output_loss: 0.9080 - gender_output_acc: 0.5665 - image_quality_output_acc: 0.5549 - age_output_acc: 0.4007 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5851 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7347 - val_gender_output_loss: 0.6848 - val_image_quality_output_loss: 0.9847 - val_age_output_loss: 1.4386 - val_weight_output_loss: 0.9763 - val_bag_output_loss: 0.9260 - val_footwear_output_loss: 0.9067 - val_pose_output_loss: 0.9351 - val_emotion_output_loss: 0.8826 - val_gender_output_acc: 0.5461 - val_image_quality_output_acc: 0.5412 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5918 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 24/96\n",
            "360/360 [==============================] - 54s 151ms/step - loss: 7.7249 - gender_output_loss: 0.6825 - image_quality_output_loss: 0.9747 - age_output_loss: 1.4226 - weight_output_loss: 0.9808 - bag_output_loss: 0.9140 - footwear_output_loss: 0.9214 - pose_output_loss: 0.9201 - emotion_output_loss: 0.9087 - gender_output_acc: 0.5655 - image_quality_output_acc: 0.5536 - age_output_acc: 0.4004 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5781 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7339 - val_gender_output_loss: 0.6887 - val_image_quality_output_loss: 0.9817 - val_age_output_loss: 1.4380 - val_weight_output_loss: 0.9765 - val_bag_output_loss: 0.9268 - val_footwear_output_loss: 0.9062 - val_pose_output_loss: 0.9343 - val_emotion_output_loss: 0.8817 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5441 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5868 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 25/96\n",
            "360/360 [==============================] - 54s 150ms/step - loss: 7.7171 - gender_output_loss: 0.6827 - image_quality_output_loss: 0.9741 - age_output_loss: 1.4223 - weight_output_loss: 0.9799 - bag_output_loss: 0.9132 - footwear_output_loss: 0.9173 - pose_output_loss: 0.9189 - emotion_output_loss: 0.9087 - gender_output_acc: 0.5663 - image_quality_output_acc: 0.5556 - age_output_acc: 0.4007 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5828 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7458 - val_gender_output_loss: 0.6853 - val_image_quality_output_loss: 0.9839 - val_age_output_loss: 1.4400 - val_weight_output_loss: 0.9756 - val_bag_output_loss: 0.9296 - val_footwear_output_loss: 0.9171 - val_pose_output_loss: 0.9343 - val_emotion_output_loss: 0.8801 - val_gender_output_acc: 0.5407 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3869 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5833 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 26/96\n",
            "360/360 [==============================] - 54s 150ms/step - loss: 7.7189 - gender_output_loss: 0.6828 - image_quality_output_loss: 0.9745 - age_output_loss: 1.4225 - weight_output_loss: 0.9799 - bag_output_loss: 0.9132 - footwear_output_loss: 0.9189 - pose_output_loss: 0.9190 - emotion_output_loss: 0.9080 - gender_output_acc: 0.5635 - image_quality_output_acc: 0.5547 - age_output_acc: 0.4004 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5854 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7575 - val_gender_output_loss: 0.6869 - val_image_quality_output_loss: 0.9866 - val_age_output_loss: 1.4411 - val_weight_output_loss: 0.9777 - val_bag_output_loss: 0.9285 - val_footwear_output_loss: 0.9194 - val_pose_output_loss: 0.9358 - val_emotion_output_loss: 0.8815 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3854 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5784 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 27/96\n",
            "360/360 [==============================] - 54s 150ms/step - loss: 7.7347 - gender_output_loss: 0.6826 - image_quality_output_loss: 0.9750 - age_output_loss: 1.4235 - weight_output_loss: 0.9813 - bag_output_loss: 0.9138 - footwear_output_loss: 0.9314 - pose_output_loss: 0.9188 - emotion_output_loss: 0.9081 - gender_output_acc: 0.5655 - image_quality_output_acc: 0.5543 - age_output_acc: 0.4004 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5788 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7777 - val_gender_output_loss: 0.6880 - val_image_quality_output_loss: 0.9861 - val_age_output_loss: 1.4415 - val_weight_output_loss: 0.9767 - val_bag_output_loss: 0.9283 - val_footwear_output_loss: 0.9342 - val_pose_output_loss: 0.9422 - val_emotion_output_loss: 0.8805 - val_gender_output_acc: 0.5397 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5719 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 28/96\n",
            "360/360 [==============================] - 54s 151ms/step - loss: 7.7297 - gender_output_loss: 0.6825 - image_quality_output_loss: 0.9732 - age_output_loss: 1.4227 - weight_output_loss: 0.9807 - bag_output_loss: 0.9137 - footwear_output_loss: 0.9286 - pose_output_loss: 0.9197 - emotion_output_loss: 0.9087 - gender_output_acc: 0.5639 - image_quality_output_acc: 0.5563 - age_output_acc: 0.4008 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5762 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7382 - val_gender_output_loss: 0.6857 - val_image_quality_output_loss: 0.9785 - val_age_output_loss: 1.4394 - val_weight_output_loss: 0.9766 - val_bag_output_loss: 0.9275 - val_footwear_output_loss: 0.9117 - val_pose_output_loss: 0.9381 - val_emotion_output_loss: 0.8807 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5427 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5818 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 29/96\n",
            "360/360 [==============================] - 54s 151ms/step - loss: 7.7183 - gender_output_loss: 0.6809 - image_quality_output_loss: 0.9735 - age_output_loss: 1.4219 - weight_output_loss: 0.9808 - bag_output_loss: 0.9130 - footwear_output_loss: 0.9227 - pose_output_loss: 0.9176 - emotion_output_loss: 0.9079 - gender_output_acc: 0.5656 - image_quality_output_acc: 0.5543 - age_output_acc: 0.4014 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5815 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7463 - val_gender_output_loss: 0.6835 - val_image_quality_output_loss: 0.9823 - val_age_output_loss: 1.4406 - val_weight_output_loss: 0.9783 - val_bag_output_loss: 0.9273 - val_footwear_output_loss: 0.9175 - val_pose_output_loss: 0.9359 - val_emotion_output_loss: 0.8809 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5446 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5853 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 30/96\n",
            "360/360 [==============================] - 54s 151ms/step - loss: 7.7144 - gender_output_loss: 0.6813 - image_quality_output_loss: 0.9751 - age_output_loss: 1.4221 - weight_output_loss: 0.9797 - bag_output_loss: 0.9124 - footwear_output_loss: 0.9172 - pose_output_loss: 0.9185 - emotion_output_loss: 0.9081 - gender_output_acc: 0.5679 - image_quality_output_acc: 0.5549 - age_output_acc: 0.4007 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5832 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7206 - val_gender_output_loss: 0.6836 - val_image_quality_output_loss: 0.9816 - val_age_output_loss: 1.4385 - val_weight_output_loss: 0.9774 - val_bag_output_loss: 0.9270 - val_footwear_output_loss: 0.8974 - val_pose_output_loss: 0.9346 - val_emotion_output_loss: 0.8807 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5441 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5992 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 31/96\n",
            "360/360 [==============================] - 54s 151ms/step - loss: 7.7016 - gender_output_loss: 0.6797 - image_quality_output_loss: 0.9738 - age_output_loss: 1.4216 - weight_output_loss: 0.9793 - bag_output_loss: 0.9128 - footwear_output_loss: 0.9099 - pose_output_loss: 0.9172 - emotion_output_loss: 0.9073 - gender_output_acc: 0.5667 - image_quality_output_acc: 0.5550 - age_output_acc: 0.4007 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5899 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7176 - val_gender_output_loss: 0.6828 - val_image_quality_output_loss: 0.9804 - val_age_output_loss: 1.4383 - val_weight_output_loss: 0.9769 - val_bag_output_loss: 0.9271 - val_footwear_output_loss: 0.8966 - val_pose_output_loss: 0.9345 - val_emotion_output_loss: 0.8812 - val_gender_output_acc: 0.5486 - val_image_quality_output_acc: 0.5441 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.6017 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 32/96\n",
            "360/360 [==============================] - 54s 151ms/step - loss: 7.6936 - gender_output_loss: 0.6792 - image_quality_output_loss: 0.9729 - age_output_loss: 1.4207 - weight_output_loss: 0.9790 - bag_output_loss: 0.9124 - footwear_output_loss: 0.9056 - pose_output_loss: 0.9163 - emotion_output_loss: 0.9075 - gender_output_acc: 0.5698 - image_quality_output_acc: 0.5552 - age_output_acc: 0.4007 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5905 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7219 - val_gender_output_loss: 0.6822 - val_image_quality_output_loss: 0.9811 - val_age_output_loss: 1.4396 - val_weight_output_loss: 0.9772 - val_bag_output_loss: 0.9270 - val_footwear_output_loss: 0.8990 - val_pose_output_loss: 0.9347 - val_emotion_output_loss: 0.8810 - val_gender_output_acc: 0.5496 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5992 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 33/96\n",
            "360/360 [==============================] - 55s 152ms/step - loss: 7.6939 - gender_output_loss: 0.6789 - image_quality_output_loss: 0.9731 - age_output_loss: 1.4204 - weight_output_loss: 0.9786 - bag_output_loss: 0.9125 - footwear_output_loss: 0.9056 - pose_output_loss: 0.9172 - emotion_output_loss: 0.9075 - gender_output_acc: 0.5712 - image_quality_output_acc: 0.5563 - age_output_acc: 0.4007 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5924 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7365 - val_gender_output_loss: 0.6834 - val_image_quality_output_loss: 0.9828 - val_age_output_loss: 1.4391 - val_weight_output_loss: 0.9761 - val_bag_output_loss: 0.9272 - val_footwear_output_loss: 0.9122 - val_pose_output_loss: 0.9351 - val_emotion_output_loss: 0.8805 - val_gender_output_acc: 0.5456 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5942 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 34/96\n",
            "360/360 [==============================] - 54s 151ms/step - loss: 7.6951 - gender_output_loss: 0.6794 - image_quality_output_loss: 0.9726 - age_output_loss: 1.4210 - weight_output_loss: 0.9786 - bag_output_loss: 0.9126 - footwear_output_loss: 0.9063 - pose_output_loss: 0.9174 - emotion_output_loss: 0.9074 - gender_output_acc: 0.5695 - image_quality_output_acc: 0.5567 - age_output_acc: 0.4006 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5924 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7725 - val_gender_output_loss: 0.6897 - val_image_quality_output_loss: 0.9864 - val_age_output_loss: 1.4407 - val_weight_output_loss: 0.9766 - val_bag_output_loss: 0.9272 - val_footwear_output_loss: 0.9308 - val_pose_output_loss: 0.9401 - val_emotion_output_loss: 0.8811 - val_gender_output_acc: 0.5437 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5729 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 35/96\n",
            "360/360 [==============================] - 54s 151ms/step - loss: 7.6986 - gender_output_loss: 0.6809 - image_quality_output_loss: 0.9746 - age_output_loss: 1.4216 - weight_output_loss: 0.9800 - bag_output_loss: 0.9127 - footwear_output_loss: 0.9019 - pose_output_loss: 0.9193 - emotion_output_loss: 0.9076 - gender_output_acc: 0.5674 - image_quality_output_acc: 0.5551 - age_output_acc: 0.4008 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5949 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7407 - val_gender_output_loss: 0.6880 - val_image_quality_output_loss: 0.9840 - val_age_output_loss: 1.4369 - val_weight_output_loss: 0.9757 - val_bag_output_loss: 0.9270 - val_footwear_output_loss: 0.9084 - val_pose_output_loss: 0.9397 - val_emotion_output_loss: 0.8811 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5432 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5938 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 36/96\n",
            "360/360 [==============================] - 54s 151ms/step - loss: 7.7210 - gender_output_loss: 0.6842 - image_quality_output_loss: 0.9762 - age_output_loss: 1.4209 - weight_output_loss: 0.9802 - bag_output_loss: 0.9130 - footwear_output_loss: 0.9155 - pose_output_loss: 0.9225 - emotion_output_loss: 0.9085 - gender_output_acc: 0.5661 - image_quality_output_acc: 0.5544 - age_output_acc: 0.4008 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5832 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7383 - val_gender_output_loss: 0.6876 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 1.4370 - val_weight_output_loss: 0.9772 - val_bag_output_loss: 0.9265 - val_footwear_output_loss: 0.9092 - val_pose_output_loss: 0.9388 - val_emotion_output_loss: 0.8806 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5382 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5789 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 37/96\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.7158 - gender_output_loss: 0.6817 - image_quality_output_loss: 0.9763 - age_output_loss: 1.4222 - weight_output_loss: 0.9806 - bag_output_loss: 0.9128 - footwear_output_loss: 0.9134 - pose_output_loss: 0.9206 - emotion_output_loss: 0.9082 - gender_output_acc: 0.5690 - image_quality_output_acc: 0.5543 - age_output_acc: 0.4007 - weight_output_acc: 0.6353 - bag_output_acc: 0.5645 - footwear_output_acc: 0.5833 - pose_output_acc: 0.6194 - emotion_output_acc: 0.7096Epoch 37/96\n",
            "360/360 [==============================] - 54s 151ms/step - loss: 7.7155 - gender_output_loss: 0.6818 - image_quality_output_loss: 0.9759 - age_output_loss: 1.4221 - weight_output_loss: 0.9805 - bag_output_loss: 0.9128 - footwear_output_loss: 0.9134 - pose_output_loss: 0.9208 - emotion_output_loss: 0.9081 - gender_output_acc: 0.5689 - image_quality_output_acc: 0.5548 - age_output_acc: 0.4007 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5833 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7532 - val_gender_output_loss: 0.6895 - val_image_quality_output_loss: 0.9816 - val_age_output_loss: 1.4392 - val_weight_output_loss: 0.9794 - val_bag_output_loss: 0.9274 - val_footwear_output_loss: 0.9160 - val_pose_output_loss: 0.9381 - val_emotion_output_loss: 0.8819 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5441 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5754 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 38/96\n",
            "360/360 [==============================] - 54s 150ms/step - loss: 7.6967 - gender_output_loss: 0.6808 - image_quality_output_loss: 0.9734 - age_output_loss: 1.4207 - weight_output_loss: 0.9795 - bag_output_loss: 0.9128 - footwear_output_loss: 0.9026 - pose_output_loss: 0.9191 - emotion_output_loss: 0.9078 - gender_output_acc: 0.5668 - image_quality_output_acc: 0.5539 - age_output_acc: 0.4007 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5942 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7441 - val_gender_output_loss: 0.6809 - val_image_quality_output_loss: 0.9800 - val_age_output_loss: 1.4386 - val_weight_output_loss: 0.9790 - val_bag_output_loss: 0.9272 - val_footwear_output_loss: 0.9198 - val_pose_output_loss: 0.9376 - val_emotion_output_loss: 0.8810 - val_gender_output_acc: 0.5843 - val_image_quality_output_acc: 0.5456 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5893 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 39/96\n",
            "360/360 [==============================] - 54s 151ms/step - loss: 7.7328 - gender_output_loss: 0.6835 - image_quality_output_loss: 0.9756 - age_output_loss: 1.4231 - weight_output_loss: 0.9796 - bag_output_loss: 0.9126 - footwear_output_loss: 0.9268 - pose_output_loss: 0.9224 - emotion_output_loss: 0.9093 - gender_output_acc: 0.5631 - image_quality_output_acc: 0.5544 - age_output_acc: 0.4010 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5751 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7294 - val_gender_output_loss: 0.6842 - val_image_quality_output_loss: 0.9810 - val_age_output_loss: 1.4370 - val_weight_output_loss: 0.9769 - val_bag_output_loss: 0.9258 - val_footwear_output_loss: 0.9041 - val_pose_output_loss: 0.9388 - val_emotion_output_loss: 0.8816 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5427 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5967 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 40/96\n",
            "360/360 [==============================] - 55s 152ms/step - loss: 7.7211 - gender_output_loss: 0.6828 - image_quality_output_loss: 0.9753 - age_output_loss: 1.4219 - weight_output_loss: 0.9796 - bag_output_loss: 0.9134 - footwear_output_loss: 0.9186 - pose_output_loss: 0.9208 - emotion_output_loss: 0.9088 - gender_output_acc: 0.5681 - image_quality_output_acc: 0.5538 - age_output_acc: 0.4007 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5806 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.8060 - val_gender_output_loss: 0.6884 - val_image_quality_output_loss: 0.9874 - val_age_output_loss: 1.4428 - val_weight_output_loss: 0.9788 - val_bag_output_loss: 0.9289 - val_footwear_output_loss: 0.9578 - val_pose_output_loss: 0.9399 - val_emotion_output_loss: 0.8819 - val_gender_output_acc: 0.5451 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5486 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 41/96\n",
            "360/360 [==============================] - 55s 152ms/step - loss: 7.7189 - gender_output_loss: 0.6836 - image_quality_output_loss: 0.9740 - age_output_loss: 1.4218 - weight_output_loss: 0.9799 - bag_output_loss: 0.9135 - footwear_output_loss: 0.9178 - pose_output_loss: 0.9198 - emotion_output_loss: 0.9084 - gender_output_acc: 0.5650 - image_quality_output_acc: 0.5547 - age_output_acc: 0.4007 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5823 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7550 - val_gender_output_loss: 0.6903 - val_image_quality_output_loss: 0.9845 - val_age_output_loss: 1.4381 - val_weight_output_loss: 0.9797 - val_bag_output_loss: 0.9283 - val_footwear_output_loss: 0.9074 - val_pose_output_loss: 0.9422 - val_emotion_output_loss: 0.8845 - val_gender_output_acc: 0.5203 - val_image_quality_output_acc: 0.5377 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5962 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 42/96\n",
            "360/360 [==============================] - 54s 151ms/step - loss: 7.7134 - gender_output_loss: 0.6816 - image_quality_output_loss: 0.9758 - age_output_loss: 1.4226 - weight_output_loss: 0.9802 - bag_output_loss: 0.9125 - footwear_output_loss: 0.9124 - pose_output_loss: 0.9199 - emotion_output_loss: 0.9084 - gender_output_acc: 0.5714 - image_quality_output_acc: 0.5539 - age_output_acc: 0.4005 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5886 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7267 - val_gender_output_loss: 0.6814 - val_image_quality_output_loss: 0.9824 - val_age_output_loss: 1.4398 - val_weight_output_loss: 0.9780 - val_bag_output_loss: 0.9264 - val_footwear_output_loss: 0.9037 - val_pose_output_loss: 0.9341 - val_emotion_output_loss: 0.8808 - val_gender_output_acc: 0.5565 - val_image_quality_output_acc: 0.5446 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5972 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 43/96\n",
            "360/360 [==============================] - 55s 151ms/step - loss: 7.7079 - gender_output_loss: 0.6815 - image_quality_output_loss: 0.9753 - age_output_loss: 1.4229 - weight_output_loss: 0.9802 - bag_output_loss: 0.9125 - footwear_output_loss: 0.9079 - pose_output_loss: 0.9191 - emotion_output_loss: 0.9085 - gender_output_acc: 0.5677 - image_quality_output_acc: 0.5547 - age_output_acc: 0.4007 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5867 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7352 - val_gender_output_loss: 0.6925 - val_image_quality_output_loss: 0.9797 - val_age_output_loss: 1.4388 - val_weight_output_loss: 0.9791 - val_bag_output_loss: 0.9266 - val_footwear_output_loss: 0.9000 - val_pose_output_loss: 0.9370 - val_emotion_output_loss: 0.8816 - val_gender_output_acc: 0.5387 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.6012 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 44/96\n",
            "360/360 [==============================] - 54s 151ms/step - loss: 7.6911 - gender_output_loss: 0.6804 - image_quality_output_loss: 0.9745 - age_output_loss: 1.4217 - weight_output_loss: 0.9791 - bag_output_loss: 0.9120 - footwear_output_loss: 0.8990 - pose_output_loss: 0.9166 - emotion_output_loss: 0.9077 - gender_output_acc: 0.5679 - image_quality_output_acc: 0.5560 - age_output_acc: 0.4008 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5976 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7015 - val_gender_output_loss: 0.6827 - val_image_quality_output_loss: 0.9800 - val_age_output_loss: 1.4365 - val_weight_output_loss: 0.9762 - val_bag_output_loss: 0.9259 - val_footwear_output_loss: 0.8871 - val_pose_output_loss: 0.9322 - val_emotion_output_loss: 0.8809 - val_gender_output_acc: 0.5536 - val_image_quality_output_acc: 0.5446 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5928 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 45/96\n",
            "360/360 [==============================] - 54s 151ms/step - loss: 7.6795 - gender_output_loss: 0.6796 - image_quality_output_loss: 0.9733 - age_output_loss: 1.4206 - weight_output_loss: 0.9786 - bag_output_loss: 0.9117 - footwear_output_loss: 0.8911 - pose_output_loss: 0.9165 - emotion_output_loss: 0.9080 - gender_output_acc: 0.5715 - image_quality_output_acc: 0.5546 - age_output_acc: 0.4008 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.6012 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7013 - val_gender_output_loss: 0.6811 - val_image_quality_output_loss: 0.9805 - val_age_output_loss: 1.4374 - val_weight_output_loss: 0.9770 - val_bag_output_loss: 0.9257 - val_footwear_output_loss: 0.8864 - val_pose_output_loss: 0.9318 - val_emotion_output_loss: 0.8815 - val_gender_output_acc: 0.5704 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.6071 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 46/96\n",
            "360/360 [==============================] - 54s 151ms/step - loss: 7.6638 - gender_output_loss: 0.6775 - image_quality_output_loss: 0.9721 - age_output_loss: 1.4201 - weight_output_loss: 0.9774 - bag_output_loss: 0.9118 - footwear_output_loss: 0.8839 - pose_output_loss: 0.9137 - emotion_output_loss: 0.9074 - gender_output_acc: 0.5784 - image_quality_output_acc: 0.5558 - age_output_acc: 0.4005 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.6030 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.6943 - val_gender_output_loss: 0.6786 - val_image_quality_output_loss: 0.9829 - val_age_output_loss: 1.4359 - val_weight_output_loss: 0.9750 - val_bag_output_loss: 0.9249 - val_footwear_output_loss: 0.8855 - val_pose_output_loss: 0.9310 - val_emotion_output_loss: 0.8805 - val_gender_output_acc: 0.5675 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.6057 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 47/96\n",
            "360/360 [==============================] - 55s 152ms/step - loss: 7.6589 - gender_output_loss: 0.6756 - image_quality_output_loss: 0.9727 - age_output_loss: 1.4198 - weight_output_loss: 0.9773 - bag_output_loss: 0.9113 - footwear_output_loss: 0.8809 - pose_output_loss: 0.9138 - emotion_output_loss: 0.9075 - gender_output_acc: 0.5816 - image_quality_output_acc: 0.5551 - age_output_acc: 0.4007 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.6007 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.6981 - val_gender_output_loss: 0.6785 - val_image_quality_output_loss: 0.9791 - val_age_output_loss: 1.4365 - val_weight_output_loss: 0.9744 - val_bag_output_loss: 0.9253 - val_footwear_output_loss: 0.8934 - val_pose_output_loss: 0.9302 - val_emotion_output_loss: 0.8808 - val_gender_output_acc: 0.5779 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.6071 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 48/96\n",
            "360/360 [==============================] - 54s 150ms/step - loss: 7.6514 - gender_output_loss: 0.6761 - image_quality_output_loss: 0.9712 - age_output_loss: 1.4183 - weight_output_loss: 0.9765 - bag_output_loss: 0.9113 - footwear_output_loss: 0.8801 - pose_output_loss: 0.9111 - emotion_output_loss: 0.9067 - gender_output_acc: 0.5823 - image_quality_output_acc: 0.5557 - age_output_acc: 0.4007 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.6049 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.6945 - val_gender_output_loss: 0.6777 - val_image_quality_output_loss: 0.9807 - val_age_output_loss: 1.4361 - val_weight_output_loss: 0.9746 - val_bag_output_loss: 0.9248 - val_footwear_output_loss: 0.8901 - val_pose_output_loss: 0.9299 - val_emotion_output_loss: 0.8805 - val_gender_output_acc: 0.5764 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.6047 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 49/96\n",
            "360/360 [==============================] - 54s 151ms/step - loss: 7.6534 - gender_output_loss: 0.6760 - image_quality_output_loss: 0.9715 - age_output_loss: 1.4184 - weight_output_loss: 0.9767 - bag_output_loss: 0.9109 - footwear_output_loss: 0.8805 - pose_output_loss: 0.9127 - emotion_output_loss: 0.9068 - gender_output_acc: 0.5794 - image_quality_output_acc: 0.5558 - age_output_acc: 0.4012 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.6037 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7055 - val_gender_output_loss: 0.6779 - val_image_quality_output_loss: 0.9829 - val_age_output_loss: 1.4355 - val_weight_output_loss: 0.9739 - val_bag_output_loss: 0.9252 - val_footwear_output_loss: 0.8999 - val_pose_output_loss: 0.9298 - val_emotion_output_loss: 0.8805 - val_gender_output_acc: 0.5744 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5997 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 50/96\n",
            "360/360 [==============================] - 54s 151ms/step - loss: 7.6531 - gender_output_loss: 0.6745 - image_quality_output_loss: 0.9721 - age_output_loss: 1.4188 - weight_output_loss: 0.9768 - bag_output_loss: 0.9107 - footwear_output_loss: 0.8796 - pose_output_loss: 0.9139 - emotion_output_loss: 0.9066 - gender_output_acc: 0.5845 - image_quality_output_acc: 0.5545 - age_output_acc: 0.4008 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.6033 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7014 - val_gender_output_loss: 0.6765 - val_image_quality_output_loss: 0.9835 - val_age_output_loss: 1.4358 - val_weight_output_loss: 0.9744 - val_bag_output_loss: 0.9247 - val_footwear_output_loss: 0.8962 - val_pose_output_loss: 0.9301 - val_emotion_output_loss: 0.8802 - val_gender_output_acc: 0.5784 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.6052 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 51/96\n",
            "360/360 [==============================] - 55s 152ms/step - loss: 7.6616 - gender_output_loss: 0.6763 - image_quality_output_loss: 0.9713 - age_output_loss: 1.4183 - weight_output_loss: 0.9776 - bag_output_loss: 0.9111 - footwear_output_loss: 0.8845 - pose_output_loss: 0.9155 - emotion_output_loss: 0.9069 - gender_output_acc: 0.5750 - image_quality_output_acc: 0.5550 - age_output_acc: 0.4004 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.6027 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7031 - val_gender_output_loss: 0.6766 - val_image_quality_output_loss: 0.9801 - val_age_output_loss: 1.4356 - val_weight_output_loss: 0.9745 - val_bag_output_loss: 0.9243 - val_footwear_output_loss: 0.9012 - val_pose_output_loss: 0.9300 - val_emotion_output_loss: 0.8808 - val_gender_output_acc: 0.5779 - val_image_quality_output_acc: 0.5446 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5972 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 52/96\n",
            "360/360 [==============================] - 55s 152ms/step - loss: 7.6614 - gender_output_loss: 0.6767 - image_quality_output_loss: 0.9713 - age_output_loss: 1.4188 - weight_output_loss: 0.9771 - bag_output_loss: 0.9111 - footwear_output_loss: 0.8853 - pose_output_loss: 0.9136 - emotion_output_loss: 0.9076 - gender_output_acc: 0.5775 - image_quality_output_acc: 0.5567 - age_output_acc: 0.4003 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.6030 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7201 - val_gender_output_loss: 0.6826 - val_image_quality_output_loss: 0.9760 - val_age_output_loss: 1.4326 - val_weight_output_loss: 0.9742 - val_bag_output_loss: 0.9261 - val_footwear_output_loss: 0.9090 - val_pose_output_loss: 0.9355 - val_emotion_output_loss: 0.8840 - val_gender_output_acc: 0.5551 - val_image_quality_output_acc: 0.5427 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5630 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 53/96\n",
            "360/360 [==============================] - 55s 151ms/step - loss: 7.6768 - gender_output_loss: 0.6782 - image_quality_output_loss: 0.9728 - age_output_loss: 1.4184 - weight_output_loss: 0.9777 - bag_output_loss: 0.9115 - footwear_output_loss: 0.8951 - pose_output_loss: 0.9149 - emotion_output_loss: 0.9081 - gender_output_acc: 0.5701 - image_quality_output_acc: 0.5548 - age_output_acc: 0.4004 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5978 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7045 - val_gender_output_loss: 0.6776 - val_image_quality_output_loss: 0.9827 - val_age_output_loss: 1.4326 - val_weight_output_loss: 0.9751 - val_bag_output_loss: 0.9249 - val_footwear_output_loss: 0.9015 - val_pose_output_loss: 0.9299 - val_emotion_output_loss: 0.8802 - val_gender_output_acc: 0.5600 - val_image_quality_output_acc: 0.5441 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5947 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 54/96\n",
            "360/360 [==============================] - 54s 150ms/step - loss: 7.6785 - gender_output_loss: 0.6773 - image_quality_output_loss: 0.9736 - age_output_loss: 1.4192 - weight_output_loss: 0.9787 - bag_output_loss: 0.9110 - footwear_output_loss: 0.8948 - pose_output_loss: 0.9162 - emotion_output_loss: 0.9077 - gender_output_acc: 0.5737 - image_quality_output_acc: 0.5556 - age_output_acc: 0.4001 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5986 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7494 - val_gender_output_loss: 0.6873 - val_image_quality_output_loss: 0.9850 - val_age_output_loss: 1.4411 - val_weight_output_loss: 0.9763 - val_bag_output_loss: 0.9257 - val_footwear_output_loss: 0.9138 - val_pose_output_loss: 0.9387 - val_emotion_output_loss: 0.8816 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5441 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5863 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 55/96\n",
            "360/360 [==============================] - 54s 150ms/step - loss: 7.7041 - gender_output_loss: 0.6820 - image_quality_output_loss: 0.9760 - age_output_loss: 1.4204 - weight_output_loss: 0.9795 - bag_output_loss: 0.9124 - footwear_output_loss: 0.9035 - pose_output_loss: 0.9216 - emotion_output_loss: 0.9087 - gender_output_acc: 0.5661 - image_quality_output_acc: 0.5549 - age_output_acc: 0.4011 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5911 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7495 - val_gender_output_loss: 0.6867 - val_image_quality_output_loss: 0.9842 - val_age_output_loss: 1.4388 - val_weight_output_loss: 0.9754 - val_bag_output_loss: 0.9265 - val_footwear_output_loss: 0.9147 - val_pose_output_loss: 0.9416 - val_emotion_output_loss: 0.8816 - val_gender_output_acc: 0.5536 - val_image_quality_output_acc: 0.5446 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5873 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 56/96\n",
            "360/360 [==============================] - 54s 150ms/step - loss: 7.7141 - gender_output_loss: 0.6825 - image_quality_output_loss: 0.9764 - age_output_loss: 1.4201 - weight_output_loss: 0.9795 - bag_output_loss: 0.9123 - footwear_output_loss: 0.9111 - pose_output_loss: 0.9229 - emotion_output_loss: 0.9093 - gender_output_acc: 0.5640 - image_quality_output_acc: 0.5551 - age_output_acc: 0.4010 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5834 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7458 - val_gender_output_loss: 0.6870 - val_image_quality_output_loss: 0.9822 - val_age_output_loss: 1.4369 - val_weight_output_loss: 0.9782 - val_bag_output_loss: 0.9263 - val_footwear_output_loss: 0.9146 - val_pose_output_loss: 0.9392 - val_emotion_output_loss: 0.8814 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5441 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5873 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 57/96\n",
            "360/360 [==============================] - 54s 151ms/step - loss: 7.7166 - gender_output_loss: 0.6823 - image_quality_output_loss: 0.9762 - age_output_loss: 1.4205 - weight_output_loss: 0.9802 - bag_output_loss: 0.9116 - footwear_output_loss: 0.9143 - pose_output_loss: 0.9226 - emotion_output_loss: 0.9088 - gender_output_acc: 0.5648 - image_quality_output_acc: 0.5559 - age_output_acc: 0.4004 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5836 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7304 - val_gender_output_loss: 0.6873 - val_image_quality_output_loss: 0.9824 - val_age_output_loss: 1.4367 - val_weight_output_loss: 0.9763 - val_bag_output_loss: 0.9248 - val_footwear_output_loss: 0.9028 - val_pose_output_loss: 0.9391 - val_emotion_output_loss: 0.8810 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5387 - val_age_output_acc: 0.3864 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5938 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 58/96\n",
            "360/360 [==============================] - 55s 152ms/step - loss: 7.7251 - gender_output_loss: 0.6827 - image_quality_output_loss: 0.9773 - age_output_loss: 1.4211 - weight_output_loss: 0.9792 - bag_output_loss: 0.9125 - footwear_output_loss: 0.9211 - pose_output_loss: 0.9227 - emotion_output_loss: 0.9085 - gender_output_acc: 0.5663 - image_quality_output_acc: 0.5546 - age_output_acc: 0.4003 - weight_output_acc: 0.6355 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5762 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7317 - val_gender_output_loss: 0.6868 - val_image_quality_output_loss: 0.9820 - val_age_output_loss: 1.4364 - val_weight_output_loss: 0.9768 - val_bag_output_loss: 0.9248 - val_footwear_output_loss: 0.9047 - val_pose_output_loss: 0.9394 - val_emotion_output_loss: 0.8808 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5446 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5923 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 59/96\n",
            "360/360 [==============================] - 55s 152ms/step - loss: 7.7194 - gender_output_loss: 0.6822 - image_quality_output_loss: 0.9765 - age_output_loss: 1.4209 - weight_output_loss: 0.9786 - bag_output_loss: 0.9125 - footwear_output_loss: 0.9184 - pose_output_loss: 0.9213 - emotion_output_loss: 0.9089 - gender_output_acc: 0.5648 - image_quality_output_acc: 0.5551 - age_output_acc: 0.4000 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5813 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7508 - val_gender_output_loss: 0.6869 - val_image_quality_output_loss: 0.9849 - val_age_output_loss: 1.4386 - val_weight_output_loss: 0.9777 - val_bag_output_loss: 0.9270 - val_footwear_output_loss: 0.9145 - val_pose_output_loss: 0.9389 - val_emotion_output_loss: 0.8822 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5873 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 60/96\n",
            "360/360 [==============================] - 54s 151ms/step - loss: 7.7165 - gender_output_loss: 0.6821 - image_quality_output_loss: 0.9768 - age_output_loss: 1.4210 - weight_output_loss: 0.9782 - bag_output_loss: 0.9125 - footwear_output_loss: 0.9159 - pose_output_loss: 0.9218 - emotion_output_loss: 0.9083 - gender_output_acc: 0.5659 - image_quality_output_acc: 0.5544 - age_output_acc: 0.4003 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5854 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7222 - val_gender_output_loss: 0.6859 - val_image_quality_output_loss: 0.9818 - val_age_output_loss: 1.4365 - val_weight_output_loss: 0.9769 - val_bag_output_loss: 0.9258 - val_footwear_output_loss: 0.8961 - val_pose_output_loss: 0.9389 - val_emotion_output_loss: 0.8803 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5918 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 61/96\n",
            "360/360 [==============================] - 55s 153ms/step - loss: 7.7065 - gender_output_loss: 0.6815 - image_quality_output_loss: 0.9761 - age_output_loss: 1.4190 - weight_output_loss: 0.9788 - bag_output_loss: 0.9121 - footwear_output_loss: 0.9099 - pose_output_loss: 0.9213 - emotion_output_loss: 0.9078 - gender_output_acc: 0.5664 - image_quality_output_acc: 0.5556 - age_output_acc: 0.4006 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5853 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7333 - val_gender_output_loss: 0.6853 - val_image_quality_output_loss: 0.9843 - val_age_output_loss: 1.4370 - val_weight_output_loss: 0.9754 - val_bag_output_loss: 0.9256 - val_footwear_output_loss: 0.9053 - val_pose_output_loss: 0.9392 - val_emotion_output_loss: 0.8812 - val_gender_output_acc: 0.5437 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5947 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 62/96\n",
            "360/360 [==============================] - 54s 151ms/step - loss: 7.7051 - gender_output_loss: 0.6814 - image_quality_output_loss: 0.9755 - age_output_loss: 1.4197 - weight_output_loss: 0.9787 - bag_output_loss: 0.9117 - footwear_output_loss: 0.9087 - pose_output_loss: 0.9213 - emotion_output_loss: 0.9081 - gender_output_acc: 0.5663 - image_quality_output_acc: 0.5556 - age_output_acc: 0.4004 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5885 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7213 - val_gender_output_loss: 0.6853 - val_image_quality_output_loss: 0.9824 - val_age_output_loss: 1.4361 - val_weight_output_loss: 0.9758 - val_bag_output_loss: 0.9248 - val_footwear_output_loss: 0.8964 - val_pose_output_loss: 0.9391 - val_emotion_output_loss: 0.8813 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5952 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 63/96\n",
            "360/360 [==============================] - 55s 152ms/step - loss: 7.6926 - gender_output_loss: 0.6805 - image_quality_output_loss: 0.9754 - age_output_loss: 1.4188 - weight_output_loss: 0.9786 - bag_output_loss: 0.9109 - footwear_output_loss: 0.9009 - pose_output_loss: 0.9199 - emotion_output_loss: 0.9077 - gender_output_acc: 0.5658 - image_quality_output_acc: 0.5550 - age_output_acc: 0.4005 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5923 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7197 - val_gender_output_loss: 0.6854 - val_image_quality_output_loss: 0.9827 - val_age_output_loss: 1.4363 - val_weight_output_loss: 0.9756 - val_bag_output_loss: 0.9244 - val_footwear_output_loss: 0.8952 - val_pose_output_loss: 0.9392 - val_emotion_output_loss: 0.8809 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5913 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 64/96\n",
            "360/360 [==============================] - 55s 153ms/step - loss: 7.6872 - gender_output_loss: 0.6807 - image_quality_output_loss: 0.9745 - age_output_loss: 1.4186 - weight_output_loss: 0.9783 - bag_output_loss: 0.9104 - footwear_output_loss: 0.8972 - pose_output_loss: 0.9200 - emotion_output_loss: 0.9075 - gender_output_acc: 0.5661 - image_quality_output_acc: 0.5567 - age_output_acc: 0.4007 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5944 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7242 - val_gender_output_loss: 0.6848 - val_image_quality_output_loss: 0.9836 - val_age_output_loss: 1.4365 - val_weight_output_loss: 0.9758 - val_bag_output_loss: 0.9244 - val_footwear_output_loss: 0.8991 - val_pose_output_loss: 0.9392 - val_emotion_output_loss: 0.8808 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5992 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 65/96\n",
            "360/360 [==============================] - 55s 152ms/step - loss: 7.6898 - gender_output_loss: 0.6798 - image_quality_output_loss: 0.9751 - age_output_loss: 1.4192 - weight_output_loss: 0.9781 - bag_output_loss: 0.9111 - footwear_output_loss: 0.8999 - pose_output_loss: 0.9194 - emotion_output_loss: 0.9073 - gender_output_acc: 0.5663 - image_quality_output_acc: 0.5559 - age_output_acc: 0.4008 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5927 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7298 - val_gender_output_loss: 0.6846 - val_image_quality_output_loss: 0.9839 - val_age_output_loss: 1.4372 - val_weight_output_loss: 0.9764 - val_bag_output_loss: 0.9250 - val_footwear_output_loss: 0.9025 - val_pose_output_loss: 0.9390 - val_emotion_output_loss: 0.8812 - val_gender_output_acc: 0.5451 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5967 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 66/96\n",
            "360/360 [==============================] - 54s 151ms/step - loss: 7.6959 - gender_output_loss: 0.6804 - image_quality_output_loss: 0.9756 - age_output_loss: 1.4194 - weight_output_loss: 0.9783 - bag_output_loss: 0.9111 - footwear_output_loss: 0.9032 - pose_output_loss: 0.9198 - emotion_output_loss: 0.9082 - gender_output_acc: 0.5660 - image_quality_output_acc: 0.5549 - age_output_acc: 0.4010 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5920 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7430 - val_gender_output_loss: 0.6846 - val_image_quality_output_loss: 0.9848 - val_age_output_loss: 1.4388 - val_weight_output_loss: 0.9771 - val_bag_output_loss: 0.9250 - val_footwear_output_loss: 0.9126 - val_pose_output_loss: 0.9390 - val_emotion_output_loss: 0.8810 - val_gender_output_acc: 0.5471 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5888 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 67/96\n",
            "360/360 [==============================] - 54s 151ms/step - loss: 7.7024 - gender_output_loss: 0.6805 - image_quality_output_loss: 0.9752 - age_output_loss: 1.4214 - weight_output_loss: 0.9792 - bag_output_loss: 0.9114 - footwear_output_loss: 0.9056 - pose_output_loss: 0.9213 - emotion_output_loss: 0.9079 - gender_output_acc: 0.5659 - image_quality_output_acc: 0.5549 - age_output_acc: 0.4008 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5898 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7457 - val_gender_output_loss: 0.6874 - val_image_quality_output_loss: 0.9853 - val_age_output_loss: 1.4390 - val_weight_output_loss: 0.9757 - val_bag_output_loss: 0.9259 - val_footwear_output_loss: 0.9135 - val_pose_output_loss: 0.9384 - val_emotion_output_loss: 0.8804 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5893 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 68/96\n",
            "360/360 [==============================] - 55s 152ms/step - loss: 7.7116 - gender_output_loss: 0.6815 - image_quality_output_loss: 0.9765 - age_output_loss: 1.4204 - weight_output_loss: 0.9787 - bag_output_loss: 0.9119 - footwear_output_loss: 0.9132 - pose_output_loss: 0.9212 - emotion_output_loss: 0.9084 - gender_output_acc: 0.5672 - image_quality_output_acc: 0.5556 - age_output_acc: 0.4005 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5837 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7330 - val_gender_output_loss: 0.6850 - val_image_quality_output_loss: 0.9837 - val_age_output_loss: 1.4378 - val_weight_output_loss: 0.9771 - val_bag_output_loss: 0.9246 - val_footwear_output_loss: 0.9027 - val_pose_output_loss: 0.9407 - val_emotion_output_loss: 0.8814 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5977 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 69/96\n",
            "360/360 [==============================] - 55s 152ms/step - loss: 7.7122 - gender_output_loss: 0.6819 - image_quality_output_loss: 0.9766 - age_output_loss: 1.4201 - weight_output_loss: 0.9792 - bag_output_loss: 0.9120 - footwear_output_loss: 0.9125 - pose_output_loss: 0.9219 - emotion_output_loss: 0.9081 - gender_output_acc: 0.5648 - image_quality_output_acc: 0.5548 - age_output_acc: 0.4005 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5868 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7748 - val_gender_output_loss: 0.6852 - val_image_quality_output_loss: 0.9868 - val_age_output_loss: 1.4383 - val_weight_output_loss: 0.9791 - val_bag_output_loss: 0.9256 - val_footwear_output_loss: 0.9384 - val_pose_output_loss: 0.9397 - val_emotion_output_loss: 0.8817 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5441 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5635 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 70/96\n",
            "360/360 [==============================] - 55s 153ms/step - loss: 7.7147 - gender_output_loss: 0.6814 - image_quality_output_loss: 0.9763 - age_output_loss: 1.4209 - weight_output_loss: 0.9793 - bag_output_loss: 0.9121 - footwear_output_loss: 0.9154 - pose_output_loss: 0.9209 - emotion_output_loss: 0.9083 - gender_output_acc: 0.5648 - image_quality_output_acc: 0.5550 - age_output_acc: 0.4005 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5819 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7874 - val_gender_output_loss: 0.6970 - val_image_quality_output_loss: 0.9878 - val_age_output_loss: 1.4395 - val_weight_output_loss: 0.9799 - val_bag_output_loss: 0.9269 - val_footwear_output_loss: 0.9359 - val_pose_output_loss: 0.9393 - val_emotion_output_loss: 0.8812 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5764 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 71/96\n",
            "360/360 [==============================] - 55s 152ms/step - loss: 7.7374 - gender_output_loss: 0.6838 - image_quality_output_loss: 0.9767 - age_output_loss: 1.4225 - weight_output_loss: 0.9805 - bag_output_loss: 0.9126 - footwear_output_loss: 0.9313 - pose_output_loss: 0.9216 - emotion_output_loss: 0.9084 - gender_output_acc: 0.5655 - image_quality_output_acc: 0.5539 - age_output_acc: 0.4004 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5746 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7810 - val_gender_output_loss: 0.6864 - val_image_quality_output_loss: 0.9884 - val_age_output_loss: 1.4407 - val_weight_output_loss: 0.9801 - val_bag_output_loss: 0.9279 - val_footwear_output_loss: 0.9317 - val_pose_output_loss: 0.9425 - val_emotion_output_loss: 0.8834 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5441 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5704 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 72/96\n",
            "360/360 [==============================] - 54s 151ms/step - loss: 7.7286 - gender_output_loss: 0.6843 - image_quality_output_loss: 0.9774 - age_output_loss: 1.4214 - weight_output_loss: 0.9801 - bag_output_loss: 0.9130 - footwear_output_loss: 0.9220 - pose_output_loss: 0.9220 - emotion_output_loss: 0.9083 - gender_output_acc: 0.5655 - image_quality_output_acc: 0.5543 - age_output_acc: 0.4017 - weight_output_acc: 0.6353 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5817 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7493 - val_gender_output_loss: 0.6859 - val_image_quality_output_loss: 0.9834 - val_age_output_loss: 1.4393 - val_weight_output_loss: 0.9781 - val_bag_output_loss: 0.9275 - val_footwear_output_loss: 0.9122 - val_pose_output_loss: 0.9411 - val_emotion_output_loss: 0.8819 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5446 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5898 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 73/96\n",
            "360/360 [==============================] - 54s 151ms/step - loss: 7.7298 - gender_output_loss: 0.6831 - image_quality_output_loss: 0.9761 - age_output_loss: 1.4218 - weight_output_loss: 0.9803 - bag_output_loss: 0.9126 - footwear_output_loss: 0.9252 - pose_output_loss: 0.9219 - emotion_output_loss: 0.9089 - gender_output_acc: 0.5657 - image_quality_output_acc: 0.5545 - age_output_acc: 0.3997 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5770 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7293 - val_gender_output_loss: 0.6859 - val_image_quality_output_loss: 0.9808 - val_age_output_loss: 1.4362 - val_weight_output_loss: 0.9761 - val_bag_output_loss: 0.9255 - val_footwear_output_loss: 0.9052 - val_pose_output_loss: 0.9390 - val_emotion_output_loss: 0.8805 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5392 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5799 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 74/96\n",
            "360/360 [==============================] - 55s 152ms/step - loss: 7.7178 - gender_output_loss: 0.6827 - image_quality_output_loss: 0.9768 - age_output_loss: 1.4208 - weight_output_loss: 0.9801 - bag_output_loss: 0.9125 - footwear_output_loss: 0.9154 - pose_output_loss: 0.9208 - emotion_output_loss: 0.9087 - gender_output_acc: 0.5658 - image_quality_output_acc: 0.5554 - age_output_acc: 0.4003 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5800 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7216 - val_gender_output_loss: 0.6857 - val_image_quality_output_loss: 0.9816 - val_age_output_loss: 1.4362 - val_weight_output_loss: 0.9770 - val_bag_output_loss: 0.9260 - val_footwear_output_loss: 0.8949 - val_pose_output_loss: 0.9386 - val_emotion_output_loss: 0.8818 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5441 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5928 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 75/96\n",
            "360/360 [==============================] - 54s 151ms/step - loss: 7.7218 - gender_output_loss: 0.6818 - image_quality_output_loss: 0.9767 - age_output_loss: 1.4213 - weight_output_loss: 0.9793 - bag_output_loss: 0.9128 - footwear_output_loss: 0.9193 - pose_output_loss: 0.9218 - emotion_output_loss: 0.9089 - gender_output_acc: 0.5659 - image_quality_output_acc: 0.5556 - age_output_acc: 0.4010 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5789 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7623 - val_gender_output_loss: 0.6866 - val_image_quality_output_loss: 0.9857 - val_age_output_loss: 1.4425 - val_weight_output_loss: 0.9789 - val_bag_output_loss: 0.9265 - val_footwear_output_loss: 0.9195 - val_pose_output_loss: 0.9398 - val_emotion_output_loss: 0.8828 - val_gender_output_acc: 0.5451 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5809 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 76/96\n",
            "360/360 [==============================] - 55s 152ms/step - loss: 7.7025 - gender_output_loss: 0.6808 - image_quality_output_loss: 0.9760 - age_output_loss: 1.4206 - weight_output_loss: 0.9788 - bag_output_loss: 0.9122 - footwear_output_loss: 0.9058 - pose_output_loss: 0.9201 - emotion_output_loss: 0.9083 - gender_output_acc: 0.5667 - image_quality_output_acc: 0.5542 - age_output_acc: 0.3999 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5915 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7392 - val_gender_output_loss: 0.6845 - val_image_quality_output_loss: 0.9841 - val_age_output_loss: 1.4383 - val_weight_output_loss: 0.9778 - val_bag_output_loss: 0.9251 - val_footwear_output_loss: 0.9069 - val_pose_output_loss: 0.9400 - val_emotion_output_loss: 0.8825 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5918 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 77/96\n",
            "360/360 [==============================] - 54s 151ms/step - loss: 7.6981 - gender_output_loss: 0.6813 - image_quality_output_loss: 0.9759 - age_output_loss: 1.4204 - weight_output_loss: 0.9789 - bag_output_loss: 0.9115 - footwear_output_loss: 0.9014 - pose_output_loss: 0.9207 - emotion_output_loss: 0.9079 - gender_output_acc: 0.5655 - image_quality_output_acc: 0.5542 - age_output_acc: 0.4010 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5924 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7214 - val_gender_output_loss: 0.6849 - val_image_quality_output_loss: 0.9839 - val_age_output_loss: 1.4365 - val_weight_output_loss: 0.9761 - val_bag_output_loss: 0.9245 - val_footwear_output_loss: 0.8948 - val_pose_output_loss: 0.9394 - val_emotion_output_loss: 0.8812 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5942 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 78/96\n",
            "360/360 [==============================] - 55s 152ms/step - loss: 7.6808 - gender_output_loss: 0.6795 - image_quality_output_loss: 0.9758 - age_output_loss: 1.4198 - weight_output_loss: 0.9783 - bag_output_loss: 0.9110 - footwear_output_loss: 0.8892 - pose_output_loss: 0.9195 - emotion_output_loss: 0.9077 - gender_output_acc: 0.5664 - image_quality_output_acc: 0.5550 - age_output_acc: 0.4009 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5995 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7217 - val_gender_output_loss: 0.6827 - val_image_quality_output_loss: 0.9827 - val_age_output_loss: 1.4390 - val_weight_output_loss: 0.9771 - val_bag_output_loss: 0.9247 - val_footwear_output_loss: 0.8957 - val_pose_output_loss: 0.9391 - val_emotion_output_loss: 0.8808 - val_gender_output_acc: 0.5437 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.6022 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 79/96\n",
            "360/360 [==============================] - 55s 154ms/step - loss: 7.6707 - gender_output_loss: 0.6795 - image_quality_output_loss: 0.9744 - age_output_loss: 1.4186 - weight_output_loss: 0.9781 - bag_output_loss: 0.9101 - footwear_output_loss: 0.8832 - pose_output_loss: 0.9192 - emotion_output_loss: 0.9074 - gender_output_acc: 0.5658 - image_quality_output_acc: 0.5556 - age_output_acc: 0.4001 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.6043 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7300 - val_gender_output_loss: 0.6840 - val_image_quality_output_loss: 0.9835 - val_age_output_loss: 1.4383 - val_weight_output_loss: 0.9770 - val_bag_output_loss: 0.9243 - val_footwear_output_loss: 0.9037 - val_pose_output_loss: 0.9382 - val_emotion_output_loss: 0.8809 - val_gender_output_acc: 0.5422 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5923 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 79/96\n",
            "Epoch 80/96\n",
            "360/360 [==============================] - 55s 154ms/step - loss: 7.6651 - gender_output_loss: 0.6782 - image_quality_output_loss: 0.9744 - age_output_loss: 1.4183 - weight_output_loss: 0.9773 - bag_output_loss: 0.9105 - footwear_output_loss: 0.8796 - pose_output_loss: 0.9195 - emotion_output_loss: 0.9074 - gender_output_acc: 0.5678 - image_quality_output_acc: 0.5560 - age_output_acc: 0.4011 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.6044 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7094 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9823 - val_age_output_loss: 1.4368 - val_weight_output_loss: 0.9762 - val_bag_output_loss: 0.9236 - val_footwear_output_loss: 0.8894 - val_pose_output_loss: 0.9382 - val_emotion_output_loss: 0.8807 - val_gender_output_acc: 0.5446 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.6022 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 81/96\n",
            "360/360 [==============================] - 55s 154ms/step - loss: 7.6663 - gender_output_loss: 0.6787 - image_quality_output_loss: 0.9742 - age_output_loss: 1.4180 - weight_output_loss: 0.9777 - bag_output_loss: 0.9105 - footwear_output_loss: 0.8815 - pose_output_loss: 0.9182 - emotion_output_loss: 0.9075 - gender_output_acc: 0.5667 - image_quality_output_acc: 0.5562 - age_output_acc: 0.4010 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.6072 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7161 - val_gender_output_loss: 0.6829 - val_image_quality_output_loss: 0.9836 - val_age_output_loss: 1.4371 - val_weight_output_loss: 0.9765 - val_bag_output_loss: 0.9239 - val_footwear_output_loss: 0.8926 - val_pose_output_loss: 0.9389 - val_emotion_output_loss: 0.8807 - val_gender_output_acc: 0.5432 - val_image_quality_output_acc: 0.5441 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.6002 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 82/96\n",
            "360/360 [==============================] - 55s 153ms/step - loss: 7.6706 - gender_output_loss: 0.6787 - image_quality_output_loss: 0.9748 - age_output_loss: 1.4192 - weight_output_loss: 0.9783 - bag_output_loss: 0.9100 - footwear_output_loss: 0.8831 - pose_output_loss: 0.9191 - emotion_output_loss: 0.9073 - gender_output_acc: 0.5667 - image_quality_output_acc: 0.5556 - age_output_acc: 0.4010 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.6057 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7560 - val_gender_output_loss: 0.6850 - val_image_quality_output_loss: 0.9861 - val_age_output_loss: 1.4397 - val_weight_output_loss: 0.9772 - val_bag_output_loss: 0.9258 - val_footwear_output_loss: 0.9227 - val_pose_output_loss: 0.9388 - val_emotion_output_loss: 0.8808 - val_gender_output_acc: 0.5456 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5794 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 83/96\n",
            "360/360 [==============================] - 54s 151ms/step - loss: 7.6833 - gender_output_loss: 0.6799 - image_quality_output_loss: 0.9750 - age_output_loss: 1.4193 - weight_output_loss: 0.9777 - bag_output_loss: 0.9113 - footwear_output_loss: 0.8931 - pose_output_loss: 0.9191 - emotion_output_loss: 0.9078 - gender_output_acc: 0.5669 - image_quality_output_acc: 0.5566 - age_output_acc: 0.3998 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5993 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7312 - val_gender_output_loss: 0.6834 - val_image_quality_output_loss: 0.9855 - val_age_output_loss: 1.4379 - val_weight_output_loss: 0.9773 - val_bag_output_loss: 0.9246 - val_footwear_output_loss: 0.9024 - val_pose_output_loss: 0.9387 - val_emotion_output_loss: 0.8813 - val_gender_output_acc: 0.5461 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5967 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 84/96\n",
            "360/360 [==============================] - 55s 152ms/step - loss: 7.6813 - gender_output_loss: 0.6799 - image_quality_output_loss: 0.9753 - age_output_loss: 1.4194 - weight_output_loss: 0.9788 - bag_output_loss: 0.9107 - footwear_output_loss: 0.8894 - pose_output_loss: 0.9199 - emotion_output_loss: 0.9080 - gender_output_acc: 0.5641 - image_quality_output_acc: 0.5548 - age_output_acc: 0.4003 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5962 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7115 - val_gender_output_loss: 0.6827 - val_image_quality_output_loss: 0.9829 - val_age_output_loss: 1.4364 - val_weight_output_loss: 0.9764 - val_bag_output_loss: 0.9238 - val_footwear_output_loss: 0.8913 - val_pose_output_loss: 0.9369 - val_emotion_output_loss: 0.8811 - val_gender_output_acc: 0.5511 - val_image_quality_output_acc: 0.5441 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.6002 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 85/96\n",
            "360/360 [==============================] - 54s 151ms/step - loss: 7.6903 - gender_output_loss: 0.6808 - image_quality_output_loss: 0.9748 - age_output_loss: 1.4193 - weight_output_loss: 0.9787 - bag_output_loss: 0.9115 - footwear_output_loss: 0.8970 - pose_output_loss: 0.9198 - emotion_output_loss: 0.9083 - gender_output_acc: 0.5648 - image_quality_output_acc: 0.5563 - age_output_acc: 0.4008 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5958 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7337 - val_gender_output_loss: 0.6868 - val_image_quality_output_loss: 0.9797 - val_age_output_loss: 1.4379 - val_weight_output_loss: 0.9777 - val_bag_output_loss: 0.9246 - val_footwear_output_loss: 0.9083 - val_pose_output_loss: 0.9382 - val_emotion_output_loss: 0.8805 - val_gender_output_acc: 0.5446 - val_image_quality_output_acc: 0.5377 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5883 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 86/96\n",
            "360/360 [==============================] - 55s 152ms/step - loss: 7.6909 - gender_output_loss: 0.6804 - image_quality_output_loss: 0.9754 - age_output_loss: 1.4196 - weight_output_loss: 0.9794 - bag_output_loss: 0.9114 - footwear_output_loss: 0.8958 - pose_output_loss: 0.9203 - emotion_output_loss: 0.9087 - gender_output_acc: 0.5628 - image_quality_output_acc: 0.5559 - age_output_acc: 0.4007 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5990 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7319 - val_gender_output_loss: 0.6871 - val_image_quality_output_loss: 0.9855 - val_age_output_loss: 1.4388 - val_weight_output_loss: 0.9765 - val_bag_output_loss: 0.9255 - val_footwear_output_loss: 0.8959 - val_pose_output_loss: 0.9402 - val_emotion_output_loss: 0.8825 - val_gender_output_acc: 0.5481 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3874 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.6002 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 87/96\n",
            "360/360 [==============================] - 55s 153ms/step - loss: 7.6963 - gender_output_loss: 0.6819 - image_quality_output_loss: 0.9762 - age_output_loss: 1.4204 - weight_output_loss: 0.9792 - bag_output_loss: 0.9116 - footwear_output_loss: 0.8979 - pose_output_loss: 0.9207 - emotion_output_loss: 0.9086 - gender_output_acc: 0.5648 - image_quality_output_acc: 0.5549 - age_output_acc: 0.3999 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5927 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7125 - val_gender_output_loss: 0.6835 - val_image_quality_output_loss: 0.9808 - val_age_output_loss: 1.4366 - val_weight_output_loss: 0.9774 - val_bag_output_loss: 0.9246 - val_footwear_output_loss: 0.8919 - val_pose_output_loss: 0.9371 - val_emotion_output_loss: 0.8805 - val_gender_output_acc: 0.5427 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.6027 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 88/96\n",
            "360/360 [==============================] - 54s 151ms/step - loss: 7.7037 - gender_output_loss: 0.6809 - image_quality_output_loss: 0.9761 - age_output_loss: 1.4211 - weight_output_loss: 0.9799 - bag_output_loss: 0.9123 - footwear_output_loss: 0.9041 - pose_output_loss: 0.9211 - emotion_output_loss: 0.9082 - gender_output_acc: 0.5650 - image_quality_output_acc: 0.5536 - age_output_acc: 0.3999 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5900 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7356 - val_gender_output_loss: 0.6848 - val_image_quality_output_loss: 0.9876 - val_age_output_loss: 1.4393 - val_weight_output_loss: 0.9779 - val_bag_output_loss: 0.9267 - val_footwear_output_loss: 0.9001 - val_pose_output_loss: 0.9393 - val_emotion_output_loss: 0.8799 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5441 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5992 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 89/96\n",
            "360/360 [==============================] - 54s 151ms/step - loss: 7.7105 - gender_output_loss: 0.6823 - image_quality_output_loss: 0.9755 - age_output_loss: 1.4210 - weight_output_loss: 0.9800 - bag_output_loss: 0.9111 - footwear_output_loss: 0.9102 - pose_output_loss: 0.9218 - emotion_output_loss: 0.9086 - gender_output_acc: 0.5642 - image_quality_output_acc: 0.5556 - age_output_acc: 0.3998 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5871 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7515 - val_gender_output_loss: 0.6838 - val_image_quality_output_loss: 0.9834 - val_age_output_loss: 1.4407 - val_weight_output_loss: 0.9782 - val_bag_output_loss: 0.9276 - val_footwear_output_loss: 0.9182 - val_pose_output_loss: 0.9380 - val_emotion_output_loss: 0.8816 - val_gender_output_acc: 0.5422 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5823 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 90/96\n",
            "360/360 [==============================] - 55s 152ms/step - loss: 7.6936 - gender_output_loss: 0.6812 - image_quality_output_loss: 0.9764 - age_output_loss: 1.4210 - weight_output_loss: 0.9793 - bag_output_loss: 0.9121 - footwear_output_loss: 0.8938 - pose_output_loss: 0.9210 - emotion_output_loss: 0.9087 - gender_output_acc: 0.5641 - image_quality_output_acc: 0.5550 - age_output_acc: 0.4003 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5990 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7201 - val_gender_output_loss: 0.6852 - val_image_quality_output_loss: 0.9802 - val_age_output_loss: 1.4378 - val_weight_output_loss: 0.9769 - val_bag_output_loss: 0.9252 - val_footwear_output_loss: 0.8959 - val_pose_output_loss: 0.9369 - val_emotion_output_loss: 0.8820 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5441 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5933 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 91/96\n",
            "360/360 [==============================] - 54s 151ms/step - loss: 7.6952 - gender_output_loss: 0.6806 - image_quality_output_loss: 0.9750 - age_output_loss: 1.4205 - weight_output_loss: 0.9789 - bag_output_loss: 0.9117 - footwear_output_loss: 0.8997 - pose_output_loss: 0.9203 - emotion_output_loss: 0.9084 - gender_output_acc: 0.5673 - image_quality_output_acc: 0.5553 - age_output_acc: 0.4003 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5957 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7108 - val_gender_output_loss: 0.6834 - val_image_quality_output_loss: 0.9850 - val_age_output_loss: 1.4359 - val_weight_output_loss: 0.9761 - val_bag_output_loss: 0.9241 - val_footwear_output_loss: 0.8868 - val_pose_output_loss: 0.9389 - val_emotion_output_loss: 0.8806 - val_gender_output_acc: 0.5575 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5977 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 92/96\n",
            "360/360 [==============================] - 55s 152ms/step - loss: 7.6810 - gender_output_loss: 0.6803 - image_quality_output_loss: 0.9747 - age_output_loss: 1.4198 - weight_output_loss: 0.9785 - bag_output_loss: 0.9108 - footwear_output_loss: 0.8884 - pose_output_loss: 0.9202 - emotion_output_loss: 0.9083 - gender_output_acc: 0.5629 - image_quality_output_acc: 0.5545 - age_output_acc: 0.4003 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.6008 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7206 - val_gender_output_loss: 0.6824 - val_image_quality_output_loss: 0.9830 - val_age_output_loss: 1.4377 - val_weight_output_loss: 0.9774 - val_bag_output_loss: 0.9240 - val_footwear_output_loss: 0.8959 - val_pose_output_loss: 0.9396 - val_emotion_output_loss: 0.8806 - val_gender_output_acc: 0.5441 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5982 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 93/96\n",
            "360/360 [==============================] - 55s 152ms/step - loss: 7.6760 - gender_output_loss: 0.6785 - image_quality_output_loss: 0.9748 - age_output_loss: 1.4191 - weight_output_loss: 0.9777 - bag_output_loss: 0.9101 - footwear_output_loss: 0.8889 - pose_output_loss: 0.9191 - emotion_output_loss: 0.9080 - gender_output_acc: 0.5661 - image_quality_output_acc: 0.5543 - age_output_acc: 0.4003 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5996 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7157 - val_gender_output_loss: 0.6818 - val_image_quality_output_loss: 0.9838 - val_age_output_loss: 1.4361 - val_weight_output_loss: 0.9764 - val_bag_output_loss: 0.9242 - val_footwear_output_loss: 0.8938 - val_pose_output_loss: 0.9389 - val_emotion_output_loss: 0.8808 - val_gender_output_acc: 0.5461 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.5977 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 94/96\n",
            "360/360 [==============================] - 55s 152ms/step - loss: 7.6726 - gender_output_loss: 0.6787 - image_quality_output_loss: 0.9744 - age_output_loss: 1.4186 - weight_output_loss: 0.9777 - bag_output_loss: 0.9105 - footwear_output_loss: 0.8856 - pose_output_loss: 0.9194 - emotion_output_loss: 0.9078 - gender_output_acc: 0.5667 - image_quality_output_acc: 0.5557 - age_output_acc: 0.4005 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.6002 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7082 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9809 - val_age_output_loss: 1.4367 - val_weight_output_loss: 0.9763 - val_bag_output_loss: 0.9242 - val_footwear_output_loss: 0.8900 - val_pose_output_loss: 0.9371 - val_emotion_output_loss: 0.8806 - val_gender_output_acc: 0.5471 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.6007 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 95/96\n",
            "360/360 [==============================] - 55s 153ms/step - loss: 7.6620 - gender_output_loss: 0.6785 - image_quality_output_loss: 0.9740 - age_output_loss: 1.4176 - weight_output_loss: 0.9780 - bag_output_loss: 0.9099 - footwear_output_loss: 0.8775 - pose_output_loss: 0.9184 - emotion_output_loss: 0.9080 - gender_output_acc: 0.5684 - image_quality_output_acc: 0.5575 - age_output_acc: 0.4010 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.6067 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7039 - val_gender_output_loss: 0.6819 - val_image_quality_output_loss: 0.9817 - val_age_output_loss: 1.4368 - val_weight_output_loss: 0.9762 - val_bag_output_loss: 0.9231 - val_footwear_output_loss: 0.8869 - val_pose_output_loss: 0.9367 - val_emotion_output_loss: 0.8805 - val_gender_output_acc: 0.5486 - val_image_quality_output_acc: 0.5432 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.6007 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n",
            "Epoch 96/96\n",
            "360/360 [==============================] - 55s 152ms/step - loss: 7.6591 - gender_output_loss: 0.6779 - image_quality_output_loss: 0.9735 - age_output_loss: 1.4169 - weight_output_loss: 0.9773 - bag_output_loss: 0.9101 - footwear_output_loss: 0.8776 - pose_output_loss: 0.9181 - emotion_output_loss: 0.9077 - gender_output_acc: 0.5660 - image_quality_output_acc: 0.5557 - age_output_acc: 0.4010 - weight_output_acc: 0.6354 - bag_output_acc: 0.5646 - footwear_output_acc: 0.6062 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7096 - val_loss: 7.7070 - val_gender_output_loss: 0.6814 - val_image_quality_output_loss: 0.9821 - val_age_output_loss: 1.4365 - val_weight_output_loss: 0.9760 - val_bag_output_loss: 0.9233 - val_footwear_output_loss: 0.8894 - val_pose_output_loss: 0.9377 - val_emotion_output_loss: 0.8807 - val_gender_output_acc: 0.5466 - val_image_quality_output_acc: 0.5432 - val_age_output_acc: 0.3879 - val_weight_output_acc: 0.6364 - val_bag_output_acc: 0.5565 - val_footwear_output_acc: 0.6007 - val_pose_output_acc: 0.6071 - val_emotion_output_acc: 0.7237\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc4d9158e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc50M4zdOZAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "results = model.evaluate_generator(valid_gen, verbose=1)\n",
        "print(results)\n",
        "#dict(zip(model.metrics_name, results))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-IFQB0idDzW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}